{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a90683d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Visible CUDA devices: 1\n",
      "Using device: cuda\n",
      "GPU: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# V5 (FINAL): N1-PUSH upgrade on top of your V4 (minimal changes)\n",
    "# Adds ONLY:\n",
    "#   (1) Soft labels near transitions (boundary soft targets)\n",
    "#   (2) Confusion-cost matrix regularizer (cost-sensitive penalty)\n",
    "#   (3) Duration-aware auxiliary head (predict remaining run-length bucket)\n",
    "# Keeps EVERYTHING else (encoder/transformer/EMA/LA-CE/aux N1/trans loss/smoothing)\n",
    "# SHHS2 stays external eval. MESA is eval-only if present in manifest.\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Visible CUDA devices:\", torch.cuda.device_count())\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf572b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 9868\n",
      "cohort  split        \n",
      "MESA    external_test    1856\n",
      "SHHS1   test              548\n",
      "        train            4380\n",
      "        val               548\n",
      "SHHS2   external_test    2536\n",
      "dtype: int64\n",
      "MESA detected | rows: 1856\n",
      "DF sizes: 4380 548 548 2536 | MESA=1856\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Paths / Manifest\n",
    "# ----------------------------\n",
    "ROOT = Path(\"/data2/Akbar1/sleep_stages_Dibatic/shhs_sleepstaging_planA/\")\n",
    "MANIFEST_PATH = ROOT / \"manifest_sleepstaging_planA.csv\"\n",
    "\n",
    "manifest = pd.read_csv(MANIFEST_PATH)\n",
    "print(\"Rows:\", len(manifest))\n",
    "print(manifest.groupby([\"cohort\",\"split\"]).size())\n",
    "\n",
    "df_train = manifest[(manifest.cohort==\"SHHS1\") & (manifest.split==\"train\")].copy()\n",
    "df_val   = manifest[(manifest.cohort==\"SHHS1\") & (manifest.split==\"val\")].copy()\n",
    "df_test  = manifest[(manifest.cohort==\"SHHS1\") & (manifest.split==\"test\")].copy()\n",
    "df_ext   = manifest[(manifest.cohort==\"SHHS2\") & (manifest.split==\"external_test\")].copy()\n",
    "\n",
    "# Optional: MESA eval-only if exists in manifest\n",
    "df_mesa = manifest[(manifest.cohort==\"MESA\")].copy() if (\"MESA\" in manifest[\"cohort\"].unique()) else None\n",
    "if df_mesa is not None and len(df_mesa) > 0:\n",
    "    # prefer explicit split name if you used it; fallback to all MESA rows\n",
    "    if \"external_test\" in df_mesa[\"split\"].unique():\n",
    "        df_mesa = df_mesa[df_mesa.split==\"external_test\"].copy()\n",
    "    print(\"MESA detected | rows:\", len(df_mesa))\n",
    "else:\n",
    "    df_mesa = None\n",
    "    print(\"MESA not detected in manifest (ok).\")\n",
    "\n",
    "print(\"DF sizes:\", len(df_train), len(df_val), len(df_test), len(df_ext), (\"| MESA=\"+str(len(df_mesa)) if df_mesa is not None else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e56ed37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenter + Normalizer ready (per-epoch z-score).\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Augment + Normalize (same as V4)\n",
    "# ----------------------------\n",
    "class EEGAugment:\n",
    "    \"\"\"\n",
    "    Safe EEG augmentations on epochs.\n",
    "    x: np.ndarray float32, shape (E, T)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 p_amp=0.5,\n",
    "                 p_noise=0.5,\n",
    "                 p_shift=0.5,\n",
    "                 p_bandstop=0.3,\n",
    "                 p_freqdrop=0.3,\n",
    "                 amp_range=(0.8, 1.2),\n",
    "                 noise_std=0.01,       # relative to per-epoch std\n",
    "                 shift_max=125,        # 1 sec at 125Hz\n",
    "                 bandstop_ranges=((49,51), (59,61)),\n",
    "                 freqdrop_max_bins=12):\n",
    "        self.p_amp = p_amp\n",
    "        self.p_noise = p_noise\n",
    "        self.p_shift = p_shift\n",
    "        self.p_bandstop = p_bandstop\n",
    "        self.p_freqdrop = p_freqdrop\n",
    "        self.amp_range = amp_range\n",
    "        self.noise_std = noise_std\n",
    "        self.shift_max = shift_max\n",
    "        self.bandstop_ranges = bandstop_ranges\n",
    "        self.freqdrop_max_bins = freqdrop_max_bins\n",
    "\n",
    "    def _amp_scale(self, x):\n",
    "        s = np.random.uniform(self.amp_range[0], self.amp_range[1])\n",
    "        return x * s\n",
    "\n",
    "    def _gaussian_noise(self, x):\n",
    "        std = np.std(x, axis=1, keepdims=True) + 1e-6\n",
    "        noise = np.random.randn(*x.shape).astype(np.float32) * (self.noise_std * std)\n",
    "        return x + noise\n",
    "\n",
    "    def _time_shift(self, x):\n",
    "        shift = np.random.randint(-self.shift_max, self.shift_max+1)\n",
    "        return np.roll(x, shift=shift, axis=1)\n",
    "\n",
    "    def _bandstop_fft(self, x, fs=125.0):\n",
    "        E, T = x.shape\n",
    "        X = np.fft.rfft(x, axis=1)\n",
    "        freqs = np.fft.rfftfreq(T, d=1.0/fs)\n",
    "        for (f1, f2) in self.bandstop_ranges:\n",
    "            mask = (freqs >= f1) & (freqs <= f2)\n",
    "            X[:, mask] = 0.0\n",
    "        y = np.fft.irfft(X, n=T, axis=1).astype(np.float32)\n",
    "        return y\n",
    "\n",
    "    def _freq_dropout(self, x):\n",
    "        E, T = x.shape\n",
    "        X = np.fft.rfft(x, axis=1)\n",
    "        Fbins = X.shape[1]\n",
    "        drop = np.random.randint(1, self.freqdrop_max_bins+1)\n",
    "        start = np.random.randint(0, max(1, Fbins - drop))\n",
    "        X[:, start:start+drop] = 0.0\n",
    "        y = np.fft.irfft(X, n=T, axis=1).astype(np.float32)\n",
    "        return y\n",
    "\n",
    "    def __call__(self, x, fs=125.0):\n",
    "        if np.random.rand() < self.p_amp:\n",
    "            x = self._amp_scale(x)\n",
    "        if np.random.rand() < self.p_noise:\n",
    "            x = self._gaussian_noise(x)\n",
    "        if np.random.rand() < self.p_shift:\n",
    "            x = self._time_shift(x)\n",
    "        if np.random.rand() < self.p_bandstop:\n",
    "            x = self._bandstop_fft(x, fs=fs)\n",
    "        if np.random.rand() < self.p_freqdrop:\n",
    "            x = self._freq_dropout(x)\n",
    "        return x\n",
    "\n",
    "def normalize_epochs_zscore(x, eps=1e-6, clip=10.0):\n",
    "    \"\"\"\n",
    "    per-epoch z-score: (x - mean) / std\n",
    "    x: (E,T) float32 -> (E,T) float32\n",
    "    \"\"\"\n",
    "    mu = np.mean(x, axis=1, keepdims=True)\n",
    "    sd = np.std(x, axis=1, keepdims=True) + eps\n",
    "    x = (x - mu) / sd\n",
    "    if clip is not None:\n",
    "        x = np.clip(x, -clip, clip)\n",
    "    return x.astype(np.float32)\n",
    "\n",
    "augment = EEGAugment()\n",
    "print(\"Augmenter + Normalizer ready (per-epoch z-score).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab83aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Labels / constants\n",
    "# ----------------------------\n",
    "LABELS = {0:\"W\", 1:\"N1\", 2:\"N2\", 3:\"N3\", 4:\"REM\"}\n",
    "NUM_CLASSES = 5\n",
    "FS = 125\n",
    "T = 3750   # 30s * 125Hz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f33c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# V5 DATASET: same as V4, but returns extra supervision:\n",
    "#   - soft_targets (L,C): boundary soft labels\n",
    "#   - dur_bucket (L,): remaining run-length bucket labels\n",
    "# ================================================================\n",
    "\n",
    "def _compute_runlength_remaining(y):\n",
    "    \"\"\"\n",
    "    y: (E,) int64\n",
    "    returns remaining run length at each t: rem[t] = how many epochs left (including t) in the current segment\n",
    "    \"\"\"\n",
    "    E = len(y)\n",
    "    rem = np.zeros((E,), dtype=np.int64)\n",
    "    t = 0\n",
    "    while t < E:\n",
    "        j = t\n",
    "        while j < E and y[j] == y[t]:\n",
    "            j += 1\n",
    "        seg_len = j - t\n",
    "        for k in range(t, j):\n",
    "            rem[k] = (j - k)  # including current epoch\n",
    "        t = j\n",
    "    return rem\n",
    "\n",
    "def _bucketize_remaining(rem, edges=(2, 5, 10, 20, 40, 80, 160)):\n",
    "    \"\"\"\n",
    "    rem: (E,) remaining run length\n",
    "    edges define bucket upper bounds.\n",
    "    returns bucket id in [0..len(edges)] (B buckets)\n",
    "    \"\"\"\n",
    "    edges = np.array(edges, dtype=np.int64)\n",
    "    b = np.zeros_like(rem, dtype=np.int64)\n",
    "    for i, r in enumerate(rem):\n",
    "        b[i] = int(np.searchsorted(edges, r, side=\"right\"))\n",
    "    return b\n",
    "\n",
    "def _make_soft_targets_boundary(y, num_classes=5, radius=2, alpha_max=0.35):\n",
    "    \"\"\"\n",
    "    Soft labels near transitions:\n",
    "      - if an epoch is within `radius` of a transition, mix current label with neighbor (prev/next).\n",
    "      - alpha decays with distance: alpha = alpha_max * (1 - dist/radius)\n",
    "    y: (E,)\n",
    "    returns soft: (E,C) float32, sums to 1\n",
    "    \"\"\"\n",
    "    E = len(y)\n",
    "    soft = np.zeros((E, num_classes), dtype=np.float32)\n",
    "    soft[np.arange(E), y] = 1.0\n",
    "\n",
    "    if E < 2 or radius <= 0:\n",
    "        return soft\n",
    "\n",
    "    trans = np.where(y[1:] != y[:-1])[0] + 1  # boundary index (start of new stage)\n",
    "    if trans.size == 0:\n",
    "        return soft\n",
    "\n",
    "    # For each boundary t (new stage begins at t), affect indices near t and t-1\n",
    "    for t in trans:\n",
    "        for dt in range(-radius, radius+1):\n",
    "            i = t + dt\n",
    "            if i < 0 or i >= E:\n",
    "                continue\n",
    "            dist = abs(dt)\n",
    "            if dist > radius:\n",
    "                continue\n",
    "            alpha = alpha_max * (1.0 - (dist / max(radius, 1)))\n",
    "            if alpha <= 0:\n",
    "                continue\n",
    "\n",
    "            # choose neighbor label towards the boundary direction\n",
    "            if i < t:\n",
    "                # before boundary => mix with next label y[t]\n",
    "                nb = y[t]\n",
    "            else:\n",
    "                # at/after boundary => mix with prev label y[t-1]\n",
    "                nb = y[t-1]\n",
    "\n",
    "            cur = y[i]\n",
    "            if nb == cur:\n",
    "                continue\n",
    "\n",
    "            # mix: (1-alpha)*onehot(cur) + alpha*onehot(nb)\n",
    "            soft[i, :] = 0.0\n",
    "            soft[i, cur] = 1.0 - alpha\n",
    "            soft[i, nb]  = alpha\n",
    "\n",
    "    # normalize (safety)\n",
    "    soft = soft / (soft.sum(axis=1, keepdims=True) + 1e-8)\n",
    "    return soft.astype(np.float32)\n",
    "\n",
    "class SleepSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    One item = one subject recording.\n",
    "\n",
    "    Returns:\n",
    "      x: (L, 1, T) float32\n",
    "      y: (L,) int64\n",
    "      mask: (L,) bool (True valid)\n",
    "      soft: (L,C) float32 (soft targets near transitions)\n",
    "      dur_bucket: (L,) int64 (duration-aware label)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, mode=\"train\",\n",
    "                 max_hours=None,\n",
    "                 min_hours=2.0,\n",
    "                 augmentor=None,\n",
    "                 exclude_unknown=True,\n",
    "                 do_normalize=True,\n",
    "                 # ---- V4 additions ----\n",
    "                 boundary_oversample_p=0.70,\n",
    "                 boundary_radius=2,\n",
    "                 # ---- V5 additions ----\n",
    "                 soft_radius=2,\n",
    "                 soft_alpha_max=0.35,\n",
    "                 dur_edges=(2,5,10,20,40,80,160)):\n",
    "        self.paths = df[\"npz_path\"].tolist()\n",
    "        self.mode = mode\n",
    "        self.max_hours = max_hours\n",
    "        self.min_hours = min_hours\n",
    "        self.augmentor = augmentor\n",
    "        self.exclude_unknown = exclude_unknown\n",
    "        self.do_normalize = do_normalize\n",
    "\n",
    "        self.boundary_oversample_p = boundary_oversample_p\n",
    "        self.boundary_radius = int(boundary_radius)\n",
    "\n",
    "        self.soft_radius = int(soft_radius)\n",
    "        self.soft_alpha_max = float(soft_alpha_max)\n",
    "        self.dur_edges = tuple(dur_edges)\n",
    "\n",
    "        print(f\"SleepSequenceDataset[{mode}] files={len(self.paths)} max_hours={self.max_hours} normalize={self.do_normalize}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def _pick_block_start_boundary_aware(self, y, L):\n",
    "        E = len(y)\n",
    "        if E <= L:\n",
    "            return 0\n",
    "\n",
    "        if np.random.rand() > self.boundary_oversample_p:\n",
    "            return np.random.randint(0, E - L + 1)\n",
    "\n",
    "        trans = np.where(y[1:] != y[:-1])[0] + 1\n",
    "        if trans.size == 0:\n",
    "            return np.random.randint(0, E - L + 1)\n",
    "\n",
    "        t = int(np.random.choice(trans))\n",
    "        start = t - (L // 2)\n",
    "        start = max(0, min(start, E - L))\n",
    "        return int(start)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        d = np.load(p, allow_pickle=True)\n",
    "\n",
    "        x = d[\"x\"].astype(np.float32)    # (E,T)\n",
    "        y = d[\"y\"].astype(np.int64)      # (E,)\n",
    "\n",
    "        if self.exclude_unknown:\n",
    "            keep = (y >= 0)\n",
    "            x = x[keep]\n",
    "            y = y[keep]\n",
    "\n",
    "        if self.do_normalize:\n",
    "            x = normalize_epochs_zscore(x, eps=1e-6, clip=10.0)\n",
    "\n",
    "        E = len(y)\n",
    "\n",
    "        # V5: compute soft targets + duration labels BEFORE block crop,\n",
    "        # then crop consistently (so supervision matches x,y)\n",
    "        soft = _make_soft_targets_boundary(y, num_classes=NUM_CLASSES,\n",
    "                                           radius=self.soft_radius,\n",
    "                                           alpha_max=self.soft_alpha_max)\n",
    "        rem = _compute_runlength_remaining(y)\n",
    "        dur_bucket = _bucketize_remaining(rem, edges=self.dur_edges)\n",
    "\n",
    "        # sample long block for training\n",
    "        if self.max_hours is not None:\n",
    "            max_L = int((self.max_hours * 3600) / 30)\n",
    "            min_L = int((self.min_hours * 3600) / 30)\n",
    "\n",
    "            L = min(max_L, E)\n",
    "            if E > L:\n",
    "                start = self._pick_block_start_boundary_aware(y, L)\n",
    "                x = x[start:start+L]\n",
    "                y = y[start:start+L]\n",
    "                soft = soft[start:start+L]\n",
    "                dur_bucket = dur_bucket[start:start+L]\n",
    "                E = L\n",
    "\n",
    "        if self.mode == \"train\" and self.augmentor is not None:\n",
    "            x = self.augmentor(x, fs=FS)\n",
    "\n",
    "        x_t = torch.from_numpy(x).unsqueeze(1)              # (E,1,T)\n",
    "        y_t = torch.from_numpy(y).long()                    # (E,)\n",
    "        soft_t = torch.from_numpy(soft).float()             # (E,C)\n",
    "        dur_t = torch.from_numpy(dur_bucket).long()         # (E,)\n",
    "        mask = torch.ones((E,), dtype=torch.bool)\n",
    "\n",
    "        return x_t, y_t, mask, soft_t, dur_t\n",
    "\n",
    "def collate_pad(batch):\n",
    "    lengths = [b[0].shape[0] for b in batch]\n",
    "    Lmax = max(lengths)\n",
    "\n",
    "    xs, ys, ms, ss, ds = [], [], [], [], []\n",
    "    for x, y, m, s, d in batch:\n",
    "        L = x.shape[0]\n",
    "        padL = Lmax - L\n",
    "        if padL > 0:\n",
    "            x = torch.cat([x, torch.zeros((padL, 1, T), dtype=x.dtype)], dim=0)\n",
    "            y = torch.cat([y, torch.zeros((padL,), dtype=y.dtype)], dim=0)\n",
    "            m = torch.cat([m, torch.zeros((padL,), dtype=torch.bool)], dim=0)\n",
    "            s = torch.cat([s, torch.zeros((padL, NUM_CLASSES), dtype=s.dtype)], dim=0)\n",
    "            d = torch.cat([d, torch.zeros((padL,), dtype=d.dtype)], dim=0)\n",
    "        xs.append(x); ys.append(y); ms.append(m); ss.append(s); ds.append(d)\n",
    "\n",
    "    x = torch.stack(xs, dim=0)  # (B,L,1,T)\n",
    "    y = torch.stack(ys, dim=0)  # (B,L)\n",
    "    m = torch.stack(ms, dim=0)  # (B,L)\n",
    "    s = torch.stack(ss, dim=0)  # (B,L,C)\n",
    "    d = torch.stack(ds, dim=0)  # (B,L)\n",
    "    return x, y, m, s, d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd9672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SleepSequenceDataset[train] files=4380 max_hours=4.0 normalize=True\n",
      "SleepSequenceDataset[eval] files=548 max_hours=None normalize=True\n",
      "SleepSequenceDataset[eval] files=548 max_hours=None normalize=True\n",
      "SleepSequenceDataset[eval] files=2536 max_hours=None normalize=True\n",
      "SleepSequenceDataset[eval] files=1856 max_hours=None normalize=True\n",
      "Batch shapes: torch.Size([2, 480, 1, 3750]) torch.Size([2, 480]) torch.Size([2, 480]) torch.Size([2, 480, 5]) torch.Size([2, 480])\n",
      "Valid tokens: 960 | y min/max(valid): 0 4\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# DataLoaders (same batching)\n",
    "# ----------------------------\n",
    "train_seq_ds = SleepSequenceDataset(\n",
    "    df_train, mode=\"train\",\n",
    "    max_hours=4.0, min_hours=2.0,\n",
    "    augmentor=augment,\n",
    "    exclude_unknown=True,\n",
    "    do_normalize=True,\n",
    "    boundary_oversample_p=0.70,\n",
    "    boundary_radius=2,\n",
    "    # V5\n",
    "    soft_radius=2,\n",
    "    soft_alpha_max=0.35,\n",
    "    dur_edges=(2,5,10,20,40,80,160),\n",
    ")\n",
    "\n",
    "val_seq_ds  = SleepSequenceDataset(df_val,  mode=\"eval\", max_hours=None, augmentor=None, do_normalize=True)\n",
    "test_seq_ds = SleepSequenceDataset(df_test, mode=\"eval\", max_hours=None, augmentor=None, do_normalize=True)\n",
    "ext_seq_ds  = SleepSequenceDataset(df_ext,  mode=\"eval\", max_hours=None, augmentor=None, do_normalize=True)\n",
    "mesa_seq_ds = SleepSequenceDataset(df_mesa, mode=\"eval\", max_hours=None, augmentor=None, do_normalize=True) if df_mesa is not None else None\n",
    "\n",
    "BATCH_SUBJ = 2\n",
    "NUM_WORKERS = 2\n",
    "PIN = True\n",
    "\n",
    "train_seq_loader = DataLoader(\n",
    "    train_seq_ds, batch_size=BATCH_SUBJ, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN,\n",
    "    collate_fn=collate_pad, persistent_workers=False\n",
    ")\n",
    "\n",
    "val_seq_loader  = DataLoader(val_seq_ds,  batch_size=1, shuffle=False, num_workers=1, pin_memory=PIN, collate_fn=collate_pad)\n",
    "test_seq_loader = DataLoader(test_seq_ds, batch_size=1, shuffle=False, num_workers=1, pin_memory=PIN, collate_fn=collate_pad)\n",
    "ext_seq_loader  = DataLoader(ext_seq_ds,  batch_size=1, shuffle=False, num_workers=1, pin_memory=PIN, collate_fn=collate_pad)\n",
    "mesa_seq_loader = DataLoader(mesa_seq_ds, batch_size=1, shuffle=False, num_workers=1, pin_memory=PIN, collate_fn=collate_pad) if mesa_seq_ds is not None else None\n",
    "\n",
    "xb, yb, mb, sb, db = next(iter(train_seq_loader))\n",
    "print(\"Batch shapes:\", xb.shape, yb.shape, mb.shape, sb.shape, db.shape)\n",
    "print(\"Valid tokens:\", int(mb.sum().item()), \"| y min/max(valid):\",\n",
    "      int(yb[mb].min().item()), int(yb[mb].max().item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "459f5073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: {'W': 951298, 'N1': 159157, 'N2': 1801005, 'N3': 557286, 'REM': 620992}\n",
      "Weights: {'W': 0.4653661355903559, 'N1': 2.781541962055294, 'N2': 0.24580824265053922, 'N3': 0.7943890104090797, 'REM': 0.7128946492947323}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Class weights (same as V4)\n",
    "# ----------------------------\n",
    "def class_counts_train(df):\n",
    "    c = Counter()\n",
    "    for p in tqdm(df[\"npz_path\"].tolist(), desc=\"Counting train labels\", leave=False):\n",
    "        d = np.load(p, allow_pickle=True)\n",
    "        y = d[\"y\"].astype(np.int64)\n",
    "        y = y[y >= 0]\n",
    "        c.update(y.tolist())\n",
    "    counts = np.array([c.get(i, 0) for i in range(NUM_CLASSES)], dtype=np.float64)\n",
    "    return counts\n",
    "\n",
    "counts = class_counts_train(df_train)\n",
    "total = counts.sum()\n",
    "weights = total / (NUM_CLASSES * np.maximum(counts, 1.0))\n",
    "weights = weights / weights.mean()\n",
    "\n",
    "print(\"Counts:\", {LABELS[i]: int(counts[i]) for i in range(NUM_CLASSES)})\n",
    "print(\"Weights:\", {LABELS[i]: float(weights[i]) for i in range(NUM_CLASSES)})\n",
    "\n",
    "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b554e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V5.1 model params (M): 22.905512\n",
      "V5.1 Local-Global settings: {'depth': 12, 'window_size': 64, 'global_every': 3}\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "# ================================================================\n",
    "# Model (V5.1): SAME V5 encoder + heads\n",
    "# + Local-Global Attention (windowed most blocks, global every k blocks)\n",
    "# Goal: better MESA generalization with small compute increase (vs full V6)\n",
    "# ================================================================\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------\n",
    "# DropPath\n",
    "# -------------------------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.drop_prob = float(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (not self.training) or self.drop_prob == 0.0:\n",
    "            return x\n",
    "        keep = 1.0 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        rand = keep + torch.rand(shape, device=x.device)\n",
    "        mask = torch.floor(rand)\n",
    "        return x / keep * mask\n",
    "\n",
    "# -------------------------\n",
    "# Residual Conv Block\n",
    "# -------------------------\n",
    "class ResConv1D(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k, s=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(c_in, c_out, k, stride=s, padding=k//2),\n",
    "            nn.BatchNorm1d(c_out),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(c_out, c_out, k, padding=k//2),\n",
    "            nn.BatchNorm1d(c_out),\n",
    "        )\n",
    "        self.skip = nn.Conv1d(c_in, c_out, 1, stride=s) if (c_in != c_out or s != 1) else nn.Identity()\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.skip(x))\n",
    "\n",
    "# -------------------------\n",
    "# Epoch Encoder (FFT SAFE)\n",
    "# -------------------------\n",
    "class EpochEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input : (B,L,1,T)\n",
    "    Output: (B,L,384)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=384):\n",
    "        super().__init__()\n",
    "        self.branch_short = ResConv1D(1, 128, k=7,  s=4)\n",
    "        self.branch_mid   = ResConv1D(1, 128, k=15, s=4)\n",
    "        self.branch_long  = ResConv1D(1, 128, k=31, s=4)\n",
    "\n",
    "        self.freq_proj = nn.Sequential(\n",
    "            nn.Linear(1876, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Linear(128*3 + 256, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, _, T_ = x.shape\n",
    "        x = x.view(B*L, 1, T_)\n",
    "\n",
    "        zs = self.branch_short(x).mean(-1)\n",
    "        zm = self.branch_mid(x).mean(-1)\n",
    "        zl = self.branch_long(x).mean(-1)\n",
    "\n",
    "        # FFT in fp32 (AMP safe)\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            xf32 = x.squeeze(1).float()\n",
    "            Xf = torch.fft.rfft(xf32, dim=-1)\n",
    "            mag = torch.abs(Xf)\n",
    "            mag = mag[:, :1876]\n",
    "            mag = torch.log1p(mag)\n",
    "            mag = mag / (mag.mean(dim=1, keepdim=True) + 1e-6)\n",
    "\n",
    "        zf = self.freq_proj(mag)\n",
    "        z = torch.cat([zs, zm, zl, zf.to(zs.dtype)], dim=-1)\n",
    "        z = self.fuse(z)\n",
    "        return z.view(B, L, -1)\n",
    "\n",
    "# -------------------------\n",
    "# RoPE helpers\n",
    "# -------------------------\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    return torch.stack((-x2, x1), dim=-1).flatten(-2)\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        assert head_dim % 2 == 0\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, H, Dh)\n",
    "        B, L, H, Dh = x.shape\n",
    "        half = Dh // 2\n",
    "        freqs = 1.0 / (self.base ** (torch.arange(half, device=x.device) / half))\n",
    "        t = torch.arange(L, device=x.device)\n",
    "        angles = torch.einsum(\"l,d->ld\", t, freqs)\n",
    "        cos = torch.cos(angles)[None, :, None, :]\n",
    "        sin = torch.sin(angles)[None, :, None, :]\n",
    "        cos = cos.repeat_interleave(2, dim=-1)\n",
    "        sin = sin.repeat_interleave(2, dim=-1)\n",
    "        return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "# ================================================================\n",
    "# V5.1 Local-Global Attention (Windowed most blocks + periodic global)\n",
    "# ================================================================\n",
    "def _windows(L, w):\n",
    "    out = []\n",
    "    s = 0\n",
    "    while s < L:\n",
    "        e = min(L, s + w)\n",
    "        out.append((s, e))\n",
    "        s = e\n",
    "    return out\n",
    "\n",
    "class MultiHeadSelfAttentionRoPE_LocalGlobal(nn.Module):\n",
    "    def __init__(self, d_model=384, n_heads=8, dropout=0.1, window_size=64):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.window_size = int(window_size)\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, 3*d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.rope = RoPE(self.d_head)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None, global_attn=False):\n",
    "        \"\"\"\n",
    "        x: (B,L,D)\n",
    "        key_padding_mask: (B,L) bool, True=valid\n",
    "        global_attn: True => full attention, else windowed attention\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        q = q.view(B, L, self.n_heads, self.d_head)\n",
    "        k = k.view(B, L, self.n_heads, self.d_head)\n",
    "        v = v.view(B, L, self.n_heads, self.d_head)\n",
    "\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "\n",
    "        q = q.transpose(1, 2)  # (B,H,L,Dh)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # ---- Global attention ----\n",
    "        if global_attn or self.window_size >= L:\n",
    "            scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B,H,L,L)\n",
    "            scores = scores.float()\n",
    "            if key_padding_mask is not None:\n",
    "                scores = scores.masked_fill(~key_padding_mask[:, None, None, :], -1e9)\n",
    "            attn = torch.softmax(scores, dim=-1)\n",
    "            attn = self.drop(attn).to(v.dtype)\n",
    "            out = attn @ v\n",
    "            out = out.transpose(1, 2).contiguous().view(B, L, D)\n",
    "            return self.proj(out)\n",
    "\n",
    "        # ---- Windowed attention ----\n",
    "        w = self.window_size\n",
    "        out = torch.zeros((B, self.n_heads, L, self.d_head), device=x.device, dtype=v.dtype)\n",
    "        for (s, e) in _windows(L, w):\n",
    "            qs = q[:, :, s:e, :]  # (B,H,w,Dh)\n",
    "            ks = k[:, :, s:e, :]\n",
    "            vs = v[:, :, s:e, :]\n",
    "\n",
    "            scores = (qs @ ks.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B,H,w,w)\n",
    "            scores = scores.float()\n",
    "\n",
    "            if key_padding_mask is not None:\n",
    "                m = key_padding_mask[:, s:e]  # (B,w)\n",
    "                scores = scores.masked_fill(~m[:, None, None, :], -1e9)\n",
    "\n",
    "            attn = torch.softmax(scores, dim=-1)\n",
    "            attn = self.drop(attn).to(vs.dtype)\n",
    "            out[:, :, s:e, :] = attn @ vs\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, D)\n",
    "        return self.proj(out)\n",
    "\n",
    "class TransformerBlockLG(nn.Module):\n",
    "    def __init__(self, d_model=384, n_heads=8, drop=0.1, drop_path=0.1, window_size=64):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadSelfAttentionRoPE_LocalGlobal(d_model, n_heads, drop, window_size=window_size)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(4*d_model, d_model),\n",
    "        )\n",
    "        self.dp = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, x, mask, global_attn=False):\n",
    "        x = x + self.dp(self.attn(self.ln1(x), key_padding_mask=mask, global_attn=global_attn))\n",
    "        x = x + self.dp(self.mlp(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "# -------------------------\n",
    "# FINAL MODEL (V5.1)\n",
    "# - same heads as V5\n",
    "# - Local-Global attention in transformer\n",
    "# -------------------------\n",
    "class HierSleepTransformerV5_1(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=5,\n",
    "        d_model=384,\n",
    "        depth=12,          # slightly deeper than V5=10 (still cheap)\n",
    "        n_heads=8,\n",
    "        dur_bins=8,\n",
    "        window_size=64,    # local window length\n",
    "        global_every=3     # every k blocks => global attention\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.dur_bins = dur_bins\n",
    "        self.depth = int(depth)\n",
    "        self.global_every = int(global_every)\n",
    "\n",
    "        self.encoder = EpochEncoder(d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlockLG(\n",
    "                d_model=d_model,\n",
    "                n_heads=n_heads,\n",
    "                drop=0.1,\n",
    "                drop_path=0.1*(i+1)/depth,\n",
    "                window_size=window_size\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.head = nn.Linear(d_model, num_classes)\n",
    "\n",
    "        # same as V5\n",
    "        self.aux_n1 = nn.Linear(d_model, 2)\n",
    "        self.aux_dur = nn.Linear(d_model, dur_bins)\n",
    "        self.trans_logits = nn.Parameter(torch.zeros(num_classes, num_classes))\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        z = self.encoder(x)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            use_global = (self.global_every > 0) and ((i % self.global_every) == 0)\n",
    "            z = blk(z, mask, global_attn=use_global)\n",
    "\n",
    "        main_logits = self.head(z)      # (B,L,C)\n",
    "        aux_logits  = self.aux_n1(z)    # (B,L,2)\n",
    "        dur_logits  = self.aux_dur(z)   # (B,L,dur_bins)\n",
    "        return main_logits, aux_logits, dur_logits\n",
    "\n",
    "# derive dur_bins from dataset bucket edges\n",
    "DUR_EDGES = (2,5,10,20,40,80,160)\n",
    "DUR_BINS = len(DUR_EDGES) + 1\n",
    "\n",
    "# Recommended defaults for MESA generalization:\n",
    "# - depth=12 (small boost)\n",
    "# - window_size=64 (try 64 or 96)\n",
    "# - global_every=3 (global attention 1/3 blocks)\n",
    "model = HierSleepTransformerV5_1(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    d_model=384,\n",
    "    depth=12,\n",
    "    n_heads=8,\n",
    "    dur_bins=DUR_BINS,\n",
    "    window_size=64,\n",
    "    global_every=3\n",
    ").to(device)\n",
    "\n",
    "print(\"V5.1 model params (M):\", sum(p.numel() for p in model.parameters()) / 1e6)\n",
    "print(\"V5.1 Local-Global settings:\", dict(depth=12, window_size=64, global_every=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dae4ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V5 switches ready.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# V5 switches: keep V4 defaults, add V5 knobs\n",
    "# ================================================================\n",
    "USE_EMA = True\n",
    "EMA_DECAY = 0.999\n",
    "\n",
    "USE_AUX_N1 = True\n",
    "AUX_N1_WEIGHT = 0.30\n",
    "\n",
    "USE_LA_CE = True\n",
    "LA_TAU = 1.0\n",
    "\n",
    "USE_HARD_NEG_N1 = True\n",
    "HARD_NEG_MULT = 2.0\n",
    "\n",
    "USE_TRANS_LOSS = True\n",
    "TRANS_LOSS_WEIGHT = 0.10\n",
    "\n",
    "# V5: soft-label loss near transitions (main logits vs soft targets)\n",
    "USE_SOFT_BOUNDARY_LOSS = True\n",
    "SOFT_BOUNDARY_WEIGHT = 0.25   # << adjust 0.15~0.35 if needed\n",
    "\n",
    "# V5: confusion-cost regularizer (expected cost under predicted distribution)\n",
    "USE_COST_MATRIX = True\n",
    "COST_WEIGHT = 0.20            # << adjust 0.10~0.30 if needed\n",
    "\n",
    "# V5: duration-aware head (run-length bucket prediction)\n",
    "USE_AUX_DUR = True\n",
    "AUX_DUR_WEIGHT = 0.15          # << adjust 0.10~0.25 if needed\n",
    "# extra boost on N1 epochs for duration head (N1 is transition-heavy)\n",
    "AUX_DUR_N1_MULT = 1.50         # << 1.2~2.0\n",
    "\n",
    "print(\"V5 switches ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f50090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost matrix ready.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Logit-Adjusted Cross Entropy (LA-CE)\n",
    "# -------------------------\n",
    "class LogitAdjustedCE(nn.Module):\n",
    "    def __init__(self, class_freq, tau=1.0):\n",
    "        super().__init__()\n",
    "        freq = torch.tensor(class_freq, dtype=torch.float32)\n",
    "        self.register_buffer(\"log_prior\", torch.log(freq / freq.sum()))\n",
    "        self.tau = float(tau)\n",
    "\n",
    "    def forward(self, logits, targets, reduction=\"none\"):\n",
    "        logits = logits + self.tau * self.log_prior\n",
    "        return F.cross_entropy(logits, targets, reduction=reduction)\n",
    "\n",
    "# -------------------------\n",
    "# Class-dependent label smoothing (vector) -- keep as V4\n",
    "# -------------------------\n",
    "def label_smoothing_nll(logits, targets, smooth_per_class):\n",
    "    \"\"\"\n",
    "    logits: (N,C)\n",
    "    targets: (N,)\n",
    "    smooth_per_class: tensor (C,) in [0,1)\n",
    "    returns per-sample loss (N,)\n",
    "    \"\"\"\n",
    "    logp = F.log_softmax(logits, dim=-1)  # (N,C)\n",
    "    nll = -logp.gather(dim=-1, index=targets.view(-1,1)).squeeze(1)  # (N,)\n",
    "    smooth = -logp.mean(dim=-1)  # (N,)\n",
    "    s = smooth_per_class[targets]  # (N,)\n",
    "    return (1 - s) * nll + s * smooth\n",
    "\n",
    "# V4 idea: smaller smoothing for N1\n",
    "smooth_vec = torch.tensor([0.02, 0.00, 0.05, 0.05, 0.02], dtype=torch.float32).to(device)\n",
    "la_ce = LogitAdjustedCE(class_freq=counts, tau=LA_TAU).to(device)\n",
    "\n",
    "# -------------------------\n",
    "# V5: boundary soft-target cross-entropy\n",
    "#   loss_soft = - sum_c soft_target[c] * log_softmax(logits)[c]\n",
    "# -------------------------\n",
    "def soft_target_ce(logits, soft_targets):\n",
    "    \"\"\"\n",
    "    logits: (N,C)\n",
    "    soft_targets: (N,C) sum to 1\n",
    "    returns per-sample loss (N,)\n",
    "    \"\"\"\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    return -(soft_targets * logp).sum(dim=-1)\n",
    "\n",
    "# -------------------------\n",
    "# V5: confusion-cost matrix regularizer\n",
    "#   expected_cost = sum_j p(j) * cost[y, j]\n",
    "# -------------------------\n",
    "def build_cost_matrix(device):\n",
    "    C = NUM_CLASSES\n",
    "    cost = torch.zeros((C, C), dtype=torch.float32, device=device)\n",
    "\n",
    "    # Base: small penalty for any wrong class\n",
    "    cost += 0.05\n",
    "    cost.fill_diagonal_(0.0)\n",
    "\n",
    "    # N1 critical confusions:\n",
    "    # true N1 predicted W or N2 => big penalty\n",
    "    cost[1, 0] = 1.00\n",
    "    cost[1, 2] = 1.00\n",
    "    # also penalize predicting N1 when true W or N2 (symmetry-ish)\n",
    "    cost[0, 1] = 0.60\n",
    "    cost[2, 1] = 0.60\n",
    "\n",
    "    # Optional: slightly penalize N2<->N3 confusion\n",
    "    cost[2, 3] = 0.20\n",
    "    cost[3, 2] = 0.20\n",
    "\n",
    "    # Optional: W<->REM confusion mild\n",
    "    cost[0, 4] = 0.15\n",
    "    cost[4, 0] = 0.15\n",
    "\n",
    "    return cost\n",
    "\n",
    "COST_MAT = build_cost_matrix(device)\n",
    "print(\"Cost matrix ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb4ecb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# EMA (same as V4)\n",
    "# -------------------------\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = float(decay)\n",
    "        self.shadow = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[name] = p.data.detach().clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            if name in self.shadow:\n",
    "                self.shadow[name].mul_(self.decay)\n",
    "                self.shadow[name].add_((1.0 - self.decay) * p.data)\n",
    "\n",
    "    def apply(self, model):\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if name in self.shadow:\n",
    "                self.backup[name] = p.data.detach().clone()\n",
    "                p.data.copy_(self.shadow[name])\n",
    "\n",
    "    def restore(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                p.data.copy_(self.backup[name])\n",
    "\n",
    "ema = EMA(model, EMA_DECAY) if USE_EMA else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dd5a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# V5 Losses: keep V4 + add:\n",
    "#   - soft boundary loss\n",
    "#   - cost matrix reg\n",
    "#   - duration aux head loss (N1-weighted)\n",
    "# ================================================================\n",
    "\n",
    "def compute_transition_loss(model, y, mask):\n",
    "    \"\"\"\n",
    "    Supervised transition matrix training:\n",
    "    minimize CE( trans_logits[y[t-1]], y[t] ) over valid adjacent pairs.\n",
    "    \"\"\"\n",
    "    B, L = y.shape\n",
    "    y_prev = y[:, :-1]\n",
    "    y_next = y[:, 1:]\n",
    "    m_pair = mask[:, :-1] & mask[:, 1:]\n",
    "\n",
    "    if m_pair.sum().item() == 0:\n",
    "        return torch.zeros((), device=y.device)\n",
    "\n",
    "    y_prev_v = y_prev[m_pair]\n",
    "    y_next_v = y_next[m_pair]\n",
    "\n",
    "    trans_logits = model.trans_logits  # (C,C)\n",
    "    logits_pair = trans_logits[y_prev_v]  # (N,C)\n",
    "    loss = F.cross_entropy(logits_pair, y_next_v)\n",
    "    return loss\n",
    "\n",
    "def masked_loss_v5(model, main_logits, aux_logits, dur_logits, y, mask, soft_targets, dur_bucket):\n",
    "    \"\"\"\n",
    "    main_logits: (B,L,C)\n",
    "    aux_logits : (B,L,2)\n",
    "    dur_logits : (B,L,DUR_BINS)\n",
    "    y         : (B,L)\n",
    "    mask      : (B,L) bool\n",
    "    soft_targets: (B,L,C)\n",
    "    dur_bucket  : (B,L)\n",
    "    \"\"\"\n",
    "    B, L, C = main_logits.shape\n",
    "\n",
    "    # flatten valid\n",
    "    logits2 = main_logits.view(B*L, C)\n",
    "    y2 = y.view(B*L)\n",
    "    m2 = mask.view(B*L)\n",
    "\n",
    "    logits_valid = logits2[m2]\n",
    "    y_valid = y2[m2]\n",
    "\n",
    "    # ---- Main loss: LA-CE + class-dependent label smoothing (same as V4) ----\n",
    "    if USE_LA_CE:\n",
    "        adj_logits = logits_valid + la_ce.tau * la_ce.log_prior\n",
    "        loss_main_vec = label_smoothing_nll(adj_logits, y_valid, smooth_vec)\n",
    "    else:\n",
    "        loss_main_vec = label_smoothing_nll(logits_valid, y_valid, smooth_vec)\n",
    "\n",
    "    # ---- V4: Hard-negative mining for N1 (when predicted W or N2) ----\n",
    "    if USE_HARD_NEG_N1:\n",
    "        with torch.no_grad():\n",
    "            pred = torch.argmax(logits_valid, dim=-1)\n",
    "            hard = (y_valid == 1) & ((pred == 0) | (pred == 2))\n",
    "        loss_main_vec = loss_main_vec * torch.where(\n",
    "            hard,\n",
    "            torch.tensor(HARD_NEG_MULT, device=logits_valid.device),\n",
    "            torch.tensor(1.0, device=logits_valid.device)\n",
    "        )\n",
    "\n",
    "    loss = loss_main_vec.mean()\n",
    "\n",
    "    # ---- V5: Soft-label boundary loss ----\n",
    "    if USE_SOFT_BOUNDARY_LOSS:\n",
    "        soft2 = soft_targets.view(B*L, C)[m2]  # (N,C)\n",
    "        # use same LA adjustment for soft targets for consistency\n",
    "        if USE_LA_CE:\n",
    "            adj_logits_soft = logits_valid + la_ce.tau * la_ce.log_prior\n",
    "            loss_soft_vec = soft_target_ce(adj_logits_soft, soft2)\n",
    "        else:\n",
    "            loss_soft_vec = soft_target_ce(logits_valid, soft2)\n",
    "        loss = loss + SOFT_BOUNDARY_WEIGHT * loss_soft_vec.mean()\n",
    "\n",
    "    # ---- V5: Confusion-cost regularizer (expected cost) ----\n",
    "    if USE_COST_MATRIX:\n",
    "        probs = torch.softmax(logits_valid.float(), dim=-1)  # (N,C)\n",
    "        # cost_row for each target: (N,C)\n",
    "        cost_row = COST_MAT[y_valid]  # (N,C)\n",
    "        expected_cost = (probs * cost_row).sum(dim=-1)  # (N,)\n",
    "        loss = loss + COST_WEIGHT * expected_cost.mean()\n",
    "\n",
    "    # ---- Aux head: N1 vs Others (same) ----\n",
    "    if USE_AUX_N1:\n",
    "        aux2 = aux_logits.view(B*L, 2)[m2]\n",
    "        y_aux = (y_valid == 1).long()\n",
    "        loss_aux = F.cross_entropy(aux2, y_aux)\n",
    "        loss = loss + AUX_N1_WEIGHT * loss_aux\n",
    "\n",
    "    # ---- V5: Duration-aware aux head ----\n",
    "    if USE_AUX_DUR:\n",
    "        dur2 = dur_logits.view(B*L, DUR_BINS)[m2]  # (N,DUR_BINS)\n",
    "        dur_t = dur_bucket.view(B*L)[m2]           # (N,)\n",
    "        # boost duration loss on N1 epochs (transition-focused)\n",
    "        w = torch.ones_like(dur_t, dtype=torch.float32, device=dur2.device)\n",
    "        w = w * torch.where(y_valid == 1, torch.tensor(AUX_DUR_N1_MULT, device=dur2.device), torch.tensor(1.0, device=dur2.device))\n",
    "        loss_dur = F.cross_entropy(dur2, dur_t, reduction=\"none\")\n",
    "        loss_dur = (loss_dur * w).mean()\n",
    "        loss = loss + AUX_DUR_WEIGHT * loss_dur\n",
    "\n",
    "    # ---- Transition loss (same) ----\n",
    "    if USE_TRANS_LOSS:\n",
    "        loss_trans = compute_transition_loss(model, y, mask)\n",
    "        loss = loss + TRANS_LOSS_WEIGHT * loss_trans\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46a43005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OneCycleLR (max_lr=5e-4)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Optimizer + OneCycle (same values as V4)\n",
    "# -------------------------\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\n",
    "\n",
    "EPOCHS = 60\n",
    "steps_per_epoch = len(train_seq_loader)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=5e-4,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    pct_start=0.15,\n",
    "    div_factor=20.0,\n",
    "    final_div_factor=100.0\n",
    ")\n",
    "scheduler_is_step_per_batch = True\n",
    "print(\"Using OneCycleLR (max_lr=5e-4)\")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1864d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Metrics helpers (same as V4)\n",
    "# -------------------------\n",
    "def _ece_from_probs(y_true, probs, n_bins=15):\n",
    "    conf = probs.max(axis=1)\n",
    "    pred = probs.argmax(axis=1)\n",
    "    acc = (pred == y_true).astype(np.float32)\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        m = (conf >= lo) & (conf < hi)\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        bin_acc = acc[m].mean()\n",
    "        bin_conf = conf[m].mean()\n",
    "        ece += (m.mean()) * abs(bin_acc - bin_conf)\n",
    "    return float(ece)\n",
    "\n",
    "def _auroc_auprc_multiclass(y_true, probs, num_classes=5):\n",
    "    Y = label_binarize(y_true, classes=list(range(num_classes)))\n",
    "    aurocs, auprcs = [], []\n",
    "    for c in range(num_classes):\n",
    "        if Y[:, c].sum() == 0:\n",
    "            continue\n",
    "        try:\n",
    "            aurocs.append(roc_auc_score(Y[:, c], probs[:, c]))\n",
    "            auprcs.append(average_precision_score(Y[:, c], probs[:, c]))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if len(aurocs) == 0:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "    return float(np.mean(aurocs)), float(np.mean(auprcs))\n",
    "\n",
    "def estimate_transition_from_train(df, num_classes=5, eps=1.0):\n",
    "    Tm = np.zeros((num_classes, num_classes), dtype=np.float64)\n",
    "    for p in tqdm(df[\"npz_path\"].tolist(), desc=\"Estimating transitions(train)\", leave=False):\n",
    "        d = np.load(p, allow_pickle=True)\n",
    "        y = d[\"y\"].astype(np.int64)\n",
    "        y = y[y >= 0]\n",
    "        if len(y) < 2:\n",
    "            continue\n",
    "        a = y[:-1]; b = y[1:]\n",
    "        for i, j in zip(a, b):\n",
    "            if 0 <= i < num_classes and 0 <= j < num_classes:\n",
    "                Tm[i, j] += 1.0\n",
    "    Tm = Tm + eps\n",
    "    Tm = Tm / Tm.sum(axis=1, keepdims=True)\n",
    "    return Tm.astype(np.float32)\n",
    "\n",
    "def viterbi_decode(log_emis, log_trans):\n",
    "    L, C = log_emis.shape\n",
    "    dp = np.zeros((L, C), dtype=np.float32)\n",
    "    back = np.zeros((L, C), dtype=np.int32)\n",
    "    dp[0] = log_emis[0]\n",
    "    for t in range(1, L):\n",
    "        scores = dp[t-1][:, None] + log_trans\n",
    "        back[t] = np.argmax(scores, axis=0)\n",
    "        dp[t] = log_emis[t] + scores[back[t], np.arange(C)]\n",
    "    path = np.zeros((L,), dtype=np.int32)\n",
    "    path[-1] = int(np.argmax(dp[-1]))\n",
    "    for t in range(L-2, -1, -1):\n",
    "        path[t] = back[t+1, path[t+1]]\n",
    "    return path\n",
    "\n",
    "# V4 learned smoothing at inference (keep)\n",
    "USE_LEARNED_SMOOTHING = True\n",
    "def apply_learned_smoothing_probs(probs, model):\n",
    "    Tm = torch.softmax(model.trans_logits, dim=1)  # (C,C)\n",
    "    return probs @ Tm\n",
    "\n",
    "# Viterbi option (keep off)\n",
    "USE_VITERBI = False\n",
    "Tmat = estimate_transition_from_train(df_train, num_classes=NUM_CLASSES, eps=1.0) if USE_VITERBI else None\n",
    "logT = np.log(Tmat + 1e-12).astype(np.float32) if USE_VITERBI else None\n",
    "\n",
    "def tensor_stats(name, t):\n",
    "    t = t.detach()\n",
    "    finite = torch.isfinite(t).all().item()\n",
    "    absmax = t.abs().max().item() if t.numel() else 0.0\n",
    "    return f\"{name}: finite={finite} absmax={absmax:.6g} dtype={t.dtype} shape={tuple(t.shape)}\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_model_params_finite(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if p is None:\n",
    "            continue\n",
    "        if not torch.isfinite(p).all():\n",
    "            return False, n\n",
    "    return True, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77493f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Eval (updated to accept new batch outputs, still reports same metrics)\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def eval_sequence(model, loader, desc=\"Eval\"):\n",
    "    model.eval()\n",
    "\n",
    "    all_true, all_pred = [], []\n",
    "    all_probs = []\n",
    "\n",
    "    all_true_v, all_pred_v = [], []\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_n = 0\n",
    "    bad_batches = 0\n",
    "    first_bad_printed = False\n",
    "\n",
    "    for bidx, (xb, yb, mb, sb, db) in enumerate(tqdm(loader, desc=desc, leave=False)):\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        mb = mb.to(device, non_blocking=True)\n",
    "        sb = sb.to(device, non_blocking=True)\n",
    "        db = db.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
    "            main_logits, aux_logits, dur_logits = model(xb, mb)\n",
    "            loss = masked_loss_v5(model, main_logits, aux_logits, dur_logits, yb, mb, sb, db)\n",
    "\n",
    "        if (not torch.isfinite(main_logits).all()) or (not torch.isfinite(loss)):\n",
    "            bad_batches += 1\n",
    "            if not first_bad_printed:\n",
    "                first_bad_printed = True\n",
    "                print(f\"\\n[WARN] {desc}: non-finite logits/loss at batch={bidx}\")\n",
    "                print(tensor_stats(\"logits\", main_logits))\n",
    "                print(\"loss:\", float(loss.item()) if torch.isfinite(loss) else loss)\n",
    "            continue\n",
    "\n",
    "        probs = torch.softmax(main_logits.float(), dim=-1)\n",
    "\n",
    "        if USE_LEARNED_SMOOTHING:\n",
    "            probs = apply_learned_smoothing_probs(probs, model)\n",
    "\n",
    "        if not torch.isfinite(probs).all():\n",
    "            bad_batches += 1\n",
    "            if not first_bad_printed:\n",
    "                first_bad_printed = True\n",
    "                print(f\"\\n[WARN] {desc}: non-finite probs at batch={bidx}\")\n",
    "                print(tensor_stats(\"logits\", main_logits))\n",
    "                print(tensor_stats(\"probs\", probs))\n",
    "            continue\n",
    "\n",
    "        pred = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        yv = yb[mb].detach().cpu().numpy()\n",
    "        pv = pred[mb].detach().cpu().numpy()\n",
    "        pr = probs[mb].detach().cpu().numpy()\n",
    "\n",
    "        if yv.size == 0:\n",
    "            continue\n",
    "\n",
    "        all_true.append(yv)\n",
    "        all_pred.append(pv)\n",
    "        all_probs.append(pr)\n",
    "\n",
    "        if USE_VITERBI:\n",
    "            probs_np = probs.detach().cpu().numpy()\n",
    "            y_np = yb.detach().cpu().numpy()\n",
    "            m_np = mb.detach().cpu().numpy()\n",
    "            for i in range(probs_np.shape[0]):\n",
    "                Li = int(m_np[i].sum())\n",
    "                if Li <= 0:\n",
    "                    continue\n",
    "                emis = probs_np[i, :Li]\n",
    "                yseq = y_np[i, :Li].astype(np.int64)\n",
    "                if not np.isfinite(emis).all():\n",
    "                    continue\n",
    "                path = viterbi_decode(np.log(emis + 1e-12).astype(np.float32), logT)\n",
    "                all_true_v.append(yseq)\n",
    "                all_pred_v.append(path.astype(np.int64))\n",
    "\n",
    "        n = int(mb.sum().item())\n",
    "        total_loss += float(loss.item()) * n\n",
    "        total_n += n\n",
    "\n",
    "    if len(all_true) == 0:\n",
    "        print(f\"\\n[STOP] {desc}: 0 valid batches collected (bad_batches={bad_batches}).\")\n",
    "        return {\n",
    "            \"loss\": float(\"nan\"),\n",
    "            \"acc\": float(\"nan\"),\n",
    "            \"macro_f1\": float(\"nan\"),\n",
    "            \"kappa\": float(\"nan\"),\n",
    "            \"AUROC\": float(\"nan\"),\n",
    "            \"AUPRC\": float(\"nan\"),\n",
    "            \"meanConf\": float(\"nan\"),\n",
    "            \"ECE\": float(\"nan\"),\n",
    "            \"f1_per_class\": {LABELS[i]: float(\"nan\") for i in range(NUM_CLASSES)},\n",
    "            \"cm\": np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64),\n",
    "            \"bad_batches\": int(bad_batches),\n",
    "        }\n",
    "\n",
    "    y_true = np.concatenate(all_true)\n",
    "    y_pred = np.concatenate(all_pred)\n",
    "    probs_all = np.concatenate(all_probs)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(NUM_CLASSES)))\n",
    "\n",
    "    auroc, auprc = _auroc_auprc_multiclass(y_true, probs_all, num_classes=NUM_CLASSES)\n",
    "    mean_conf = float(probs_all.max(axis=1).mean())\n",
    "    ece = _ece_from_probs(y_true, probs_all, n_bins=15)\n",
    "\n",
    "    f1_per = {}\n",
    "    for i in range(NUM_CLASSES):\n",
    "        f1_per[LABELS[i]] = float(f1_score((y_true==i).astype(int), (y_pred==i).astype(int)))\n",
    "\n",
    "    out = {\n",
    "        \"loss\": total_loss / max(total_n, 1),\n",
    "        \"acc\": float(acc),\n",
    "        \"macro_f1\": float(mf1),\n",
    "        \"kappa\": float(kappa),\n",
    "        \"AUROC\": auroc,\n",
    "        \"AUPRC\": auprc,\n",
    "        \"meanConf\": mean_conf,\n",
    "        \"ECE\": ece,\n",
    "        \"f1_per_class\": f1_per,\n",
    "        \"cm\": cm,\n",
    "        \"bad_batches\": int(bad_batches),\n",
    "    }\n",
    "\n",
    "    if USE_VITERBI and len(all_true_v) > 0:\n",
    "        yt = np.concatenate(all_true_v)\n",
    "        yp = np.concatenate(all_pred_v)\n",
    "        out[\"viterbi_acc\"] = float(accuracy_score(yt, yp))\n",
    "        out[\"viterbi_macro_f1\"] = float(f1_score(yt, yp, average=\"macro\"))\n",
    "        out[\"viterbi_kappa\"] = float(cohen_kappa_score(yt, yp))\n",
    "        out[\"viterbi_cm\"] = confusion_matrix(yt, yp, labels=list(range(NUM_CLASSES)))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80bd2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Train one epoch (updated batch tuple + V5 loss)\n",
    "# -------------------------\n",
    "def train_one_epoch(model, loader, epoch, grad_clip=1.0, droppath_target=0.10, droppath_warm_epochs=10):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_seen = 0\n",
    "\n",
    "    # DropPath warmup (keep)\n",
    "    warm = min(1.0, epoch / max(1, droppath_warm_epochs))\n",
    "    for blk in model.blocks:\n",
    "        blk.dp.drop_prob = droppath_target * warm\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Train epoch {epoch}\", leave=False)\n",
    "    for step, (xb, yb, mb, sb, db) in enumerate(pbar):\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        mb = mb.to(device, non_blocking=True)\n",
    "        sb = sb.to(device, non_blocking=True)\n",
    "        db = db.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
    "            main_logits, aux_logits, dur_logits = model(xb, mb)\n",
    "            loss = masked_loss_v5(model, main_logits, aux_logits, dur_logits, yb, mb, sb, db)\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            print(\"\\n[STOP] Non-finite loss detected:\", float(loss.item()))\n",
    "            print(tensor_stats(\"logits\", main_logits))\n",
    "            ok, bad = check_model_params_finite(model)\n",
    "            print(\"params finite:\", ok, \"| first bad:\", bad)\n",
    "            return float(\"nan\")\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if scheduler_is_step_per_batch:\n",
    "            scheduler.step()\n",
    "\n",
    "        if USE_EMA:\n",
    "            ema.update(model)\n",
    "\n",
    "        n = int(mb.sum().item())\n",
    "        running_loss += float(loss.item()) * n\n",
    "        n_seen += n\n",
    "        pbar.set_postfix(loss=running_loss/max(n_seen,1), lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return running_loss / max(n_seen, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10d8e5e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 1:   0%|                                   | 0/2190 [00:00<?, ?it/s]/usr/local/anaconda3/envs/akbar1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01 | train_loss=1.3762\n",
      "  VAL   : loss=1.2399 acc=0.7872 macroF1=0.6513 kappa=0.6938 AUROC=0.9473 AUPRC=0.7838 meanConf=0.2081 ECE=0.5791\n",
      "  F1/class: {'W': 0.8225161576933548, 'N1': 0.05058552087414566, 'N2': 0.8033134343332999, 'N3': 0.7542364701826956, 'REM': 0.8258525709836858}\n",
      "  TEST1 : loss=1.2185 acc=0.7935 macroF1=0.6554 kappa=0.7028 AUROC=0.9509 AUPRC=0.7917 meanConf=0.2081 ECE=0.5855\n",
      "  SHHS2 : loss=1.2955 acc=0.7675 macroF1=0.6282 kappa=0.6688 AUROC=0.9456 AUPRC=0.7761 meanConf=0.2077 ECE=0.5598\n",
      "  MESA  : loss=1.7274 acc=0.6964 macroF1=0.5758 kappa=0.5577 AUROC=0.8980 AUPRC=0.6962 meanConf=0.2069 ECE=0.4895\n",
      "  MESA F1/class: {'W': 0.7600170124661046, 'N1': 0.0027893560708116077, 'N2': 0.7053987029187828, 'N3': 0.6437907217558705, 'REM': 0.7672267376707401}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.6513)\n",
      "  ? Added to VAL Top-K: VALBEST_ep001_valF1_0.6513.pt\n",
      "  ? Saved BEST_MESA: BEST_MESA_macroF1.pt (mesa_macroF1=0.5758)\n",
      "  ? Snapshot MESA best: MESABEST_ep001_mesaF1_0.5758.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02 | train_loss=1.1840\n",
      "  VAL   : loss=1.0878 acc=0.8294 macroF1=0.7371 kappa=0.7574 AUROC=0.9583 AUPRC=0.8177 meanConf=0.2318 ECE=0.5976\n",
      "  F1/class: {'W': 0.8929749536464091, 'N1': 0.32192874692874696, 'N2': 0.8415611380865352, 'N3': 0.7665224441816495, 'REM': 0.8625898171277022}\n",
      "  TEST1 : loss=1.0563 acc=0.8376 macroF1=0.7423 kappa=0.7687 AUROC=0.9620 AUPRC=0.8281 meanConf=0.2318 ECE=0.6058\n",
      "  SHHS2 : loss=1.0430 acc=0.8366 macroF1=0.7236 kappa=0.7678 AUROC=0.9621 AUPRC=0.8265 meanConf=0.2308 ECE=0.6058\n",
      "  MESA  : loss=1.5583 acc=0.7224 macroF1=0.6057 kappa=0.5956 AUROC=0.9181 AUPRC=0.7332 meanConf=0.2270 ECE=0.4954\n",
      "  MESA F1/class: {'W': 0.7874290416156475, 'N1': 0.1026176631099569, 'N2': 0.7370577437978761, 'N3': 0.610963880057253, 'REM': 0.7902263250543472}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7371)\n",
      "  ? Added to VAL Top-K: VALBEST_ep002_valF1_0.7371.pt\n",
      "  ? Saved BEST_MESA: BEST_MESA_macroF1.pt (mesa_macroF1=0.6057)\n",
      "  ? Snapshot MESA best: MESABEST_ep002_mesaF1_0.6057.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 03 | train_loss=1.1380\n",
      "  VAL   : loss=1.0352 acc=0.8370 macroF1=0.7680 kappa=0.7721 AUROC=0.9611 AUPRC=0.8254 meanConf=0.2797 ECE=0.5573\n",
      "  F1/class: {'W': 0.8984920431490275, 'N1': 0.44173441734417346, 'N2': 0.8473815833169547, 'N3': 0.7813822284908323, 'REM': 0.8711978877327458}\n",
      "  TEST1 : loss=1.0014 acc=0.8450 macroF1=0.7741 kappa=0.7831 AUROC=0.9648 AUPRC=0.8363 meanConf=0.2798 ECE=0.5652\n",
      "  SHHS2 : loss=0.9763 acc=0.8460 macroF1=0.7592 kappa=0.7843 AUROC=0.9653 AUPRC=0.8356 meanConf=0.2782 ECE=0.5678\n",
      "  MESA  : loss=1.5273 acc=0.7462 macroF1=0.6558 kappa=0.6351 AUROC=0.9203 AUPRC=0.7437 meanConf=0.2696 ECE=0.4766\n",
      "  MESA F1/class: {'W': 0.8132645434160393, 'N1': 0.2525295360238428, 'N2': 0.7573898988588511, 'N3': 0.6503484813853286, 'REM': 0.805698821154993}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7680)\n",
      "  ? Added to VAL Top-K: VALBEST_ep003_valF1_0.7680.pt\n",
      "  ? Saved BEST_MESA: BEST_MESA_macroF1.pt (mesa_macroF1=0.6558)\n",
      "  ? Snapshot MESA best: MESABEST_ep003_mesaF1_0.6558.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 04 | train_loss=1.1066\n",
      "  VAL   : loss=0.9896 acc=0.8369 macroF1=0.7786 kappa=0.7746 AUROC=0.9630 AUPRC=0.8283 meanConf=0.3612 ECE=0.4756\n",
      "  F1/class: {'W': 0.9020108660360613, 'N1': 0.4918646636651362, 'N2': 0.8445191068778235, 'N3': 0.7820647696305216, 'REM': 0.8724920607567515}\n",
      "  TEST1 : loss=0.9558 acc=0.8449 macroF1=0.7852 kappa=0.7856 AUROC=0.9665 AUPRC=0.8388 meanConf=0.3614 ECE=0.4836\n",
      "  SHHS2 : loss=0.9286 acc=0.8479 macroF1=0.7742 kappa=0.7891 AUROC=0.9673 AUPRC=0.8408 meanConf=0.3577 ECE=0.4902\n",
      "  MESA  : loss=1.5120 acc=0.7448 macroF1=0.6647 kappa=0.6369 AUROC=0.9198 AUPRC=0.7425 meanConf=0.3396 ECE=0.4053\n",
      "  MESA F1/class: {'W': 0.8105911841460651, 'N1': 0.30588381231051004, 'N2': 0.7568977574893391, 'N3': 0.6500011293929352, 'REM': 0.8000120698132196}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7786)\n",
      "  ? Added to VAL Top-K: VALBEST_ep004_valF1_0.7786.pt\n",
      "  ? Saved BEST_MESA: BEST_MESA_macroF1.pt (mesa_macroF1=0.6647)\n",
      "  ? Snapshot MESA best: MESABEST_ep004_mesaF1_0.6647.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 05 | train_loss=1.0780\n",
      "  VAL   : loss=0.9641 acc=0.8345 macroF1=0.7800 kappa=0.7731 AUROC=0.9644 AUPRC=0.8279 meanConf=0.4726 ECE=0.3619\n",
      "  F1/class: {'W': 0.9037463144389026, 'N1': 0.5000596350335944, 'N2': 0.8402708386863467, 'N3': 0.7835339352448146, 'REM': 0.8723571353480635}\n",
      "  TEST1 : loss=0.9276 acc=0.8437 macroF1=0.7883 kappa=0.7854 AUROC=0.9677 AUPRC=0.8389 meanConf=0.4724 ECE=0.3713\n",
      "  SHHS2 : loss=0.8907 acc=0.8503 macroF1=0.7822 kappa=0.7936 AUROC=0.9694 AUPRC=0.8435 meanConf=0.4649 ECE=0.3853\n",
      "  MESA  : loss=1.4128 acc=0.7669 macroF1=0.6881 kappa=0.6691 AUROC=0.9258 AUPRC=0.7479 meanConf=0.4349 ECE=0.3320\n",
      "  MESA F1/class: {'W': 0.8460318673463424, 'N1': 0.3657237301305098, 'N2': 0.7753024636655753, 'N3': 0.6455588243749582, 'REM': 0.8078234968693556}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7800)\n",
      "  ? Added to VAL Top-K: VALBEST_ep005_valF1_0.7800.pt\n",
      "  ? Saved BEST_MESA: BEST_MESA_macroF1.pt (mesa_macroF1=0.6881)\n",
      "  ? Snapshot MESA best: MESABEST_ep005_mesaF1_0.6881.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 06 | train_loss=1.0460\n",
      "  VAL   : loss=0.9357 acc=0.8375 macroF1=0.7820 kappa=0.7766 AUROC=0.9651 AUPRC=0.8313 meanConf=0.5777 ECE=0.2598\n",
      "  F1/class: {'W': 0.9034265213096001, 'N1': 0.4985085684206422, 'N2': 0.846469164720574, 'N3': 0.7884219039419792, 'REM': 0.8729646044563172}\n",
      "  TEST1 : loss=0.9036 acc=0.8455 macroF1=0.7890 kappa=0.7874 AUROC=0.9678 AUPRC=0.8409 meanConf=0.5776 ECE=0.2679\n",
      "  SHHS2 : loss=0.8625 acc=0.8537 macroF1=0.7846 kappa=0.7983 AUROC=0.9697 AUPRC=0.8452 meanConf=0.5666 ECE=0.2871\n",
      "  MESA  : loss=1.4167 acc=0.7601 macroF1=0.6862 kappa=0.6615 AUROC=0.9244 AUPRC=0.7512 meanConf=0.5305 ECE=0.2296\n",
      "  MESA F1/class: {'W': 0.8325163511777528, 'N1': 0.3766883237306729, 'N2': 0.7745656877173027, 'N3': 0.6530745283771919, 'REM': 0.7941502112615331}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7820)\n",
      "  ? Added to VAL Top-K: VALBEST_ep006_valF1_0.7820.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 07 | train_loss=1.0325\n",
      "  VAL   : loss=0.9204 acc=0.8346 macroF1=0.7814 kappa=0.7738 AUROC=0.9661 AUPRC=0.8339 meanConf=0.6424 ECE=0.1921\n",
      "  F1/class: {'W': 0.9073813002503439, 'N1': 0.5047862156987875, 'N2': 0.8376214308153102, 'N3': 0.7824870524306614, 'REM': 0.8748836183662468}\n",
      "  TEST1 : loss=0.8867 acc=0.8431 macroF1=0.7892 kappa=0.7853 AUROC=0.9691 AUPRC=0.8442 meanConf=0.6420 ECE=0.2011\n",
      "  SHHS2 : loss=0.8611 acc=0.8492 macroF1=0.7829 kappa=0.7926 AUROC=0.9701 AUPRC=0.8470 meanConf=0.6287 ECE=0.2204\n",
      "  MESA  : loss=1.4491 acc=0.7500 macroF1=0.6831 kappa=0.6452 AUROC=0.9202 AUPRC=0.7455 meanConf=0.5743 ECE=0.1757\n",
      "  MESA F1/class: {'W': 0.817154839480285, 'N1': 0.3823357842037449, 'N2': 0.7545487619038902, 'N3': 0.6475781936094035, 'REM': 0.8139909851161258}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 08 | train_loss=1.0274\n",
      "  VAL   : loss=0.9186 acc=0.8393 macroF1=0.7845 kappa=0.7785 AUROC=0.9653 AUPRC=0.8343 meanConf=0.6701 ECE=0.1692\n",
      "  F1/class: {'W': 0.9028583783783785, 'N1': 0.5075208354252435, 'N2': 0.8475247617355194, 'N3': 0.7909779976839667, 'REM': 0.8734851568147443}\n",
      "  TEST1 : loss=0.8838 acc=0.8496 macroF1=0.7933 kappa=0.7925 AUROC=0.9684 AUPRC=0.8444 meanConf=0.6700 ECE=0.1795\n",
      "  SHHS2 : loss=0.8770 acc=0.8500 macroF1=0.7836 kappa=0.7927 AUROC=0.9684 AUPRC=0.8449 meanConf=0.6547 ECE=0.1953\n",
      "  MESA  : loss=1.4269 acc=0.7627 macroF1=0.6879 kappa=0.6616 AUROC=0.9234 AUPRC=0.7529 meanConf=0.6108 ECE=0.1519\n",
      "  MESA F1/class: {'W': 0.8318389690173232, 'N1': 0.37510108243455215, 'N2': 0.7725722977736569, 'N3': 0.6469995106689007, 'REM': 0.8129401002701763}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7845)\n",
      "  ? Added to VAL Top-K: VALBEST_ep008_valF1_0.7845.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 09 | train_loss=1.0120\n",
      "  VAL   : loss=0.9047 acc=0.8388 macroF1=0.7850 kappa=0.7791 AUROC=0.9664 AUPRC=0.8374 meanConf=0.6874 ECE=0.1515\n",
      "  F1/class: {'W': 0.9079161317928969, 'N1': 0.5097196729517771, 'N2': 0.842656425180697, 'N3': 0.7879976588087322, 'REM': 0.876646121925917}\n",
      "  TEST1 : loss=0.8696 acc=0.8475 macroF1=0.7930 kappa=0.7909 AUROC=0.9694 AUPRC=0.8475 meanConf=0.6871 ECE=0.1605\n",
      "  SHHS2 : loss=0.8370 acc=0.8545 macroF1=0.7870 kappa=0.7995 AUROC=0.9702 AUPRC=0.8488 meanConf=0.6785 ECE=0.1759\n",
      "  MESA  : loss=1.3487 acc=0.7877 macroF1=0.7011 kappa=0.6974 AUROC=0.9280 AUPRC=0.7594 meanConf=0.6289 ECE=0.1588\n",
      "  MESA F1/class: {'W': 0.8748322708648105, 'N1': 0.3726567854119354, 'N2': 0.7930000597405839, 'N3': 0.6460641801596745, 'REM': 0.8188675796140673}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7850)\n",
      "  ? Added to VAL Top-K: VALBEST_ep009_valF1_0.7850.pt\n",
      "  ? Saved BEST_MESA: BEST_MESA_macroF1.pt (mesa_macroF1=0.7011)\n",
      "  ? Snapshot MESA best: MESABEST_ep009_mesaF1_0.7011.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | train_loss=1.0009\n",
      "  VAL   : loss=0.9000 acc=0.8421 macroF1=0.7871 kappa=0.7828 AUROC=0.9667 AUPRC=0.8385 meanConf=0.6893 ECE=0.1527\n",
      "  F1/class: {'W': 0.9069515218289491, 'N1': 0.5078441347034287, 'N2': 0.8491428759885136, 'N3': 0.795769158920646, 'REM': 0.8757183689140516}\n",
      "  TEST1 : loss=0.8642 acc=0.8513 macroF1=0.7954 kappa=0.7954 AUROC=0.9697 AUPRC=0.8491 meanConf=0.6896 ECE=0.1617\n",
      "  SHHS2 : loss=0.8365 acc=0.8581 macroF1=0.7901 kappa=0.8041 AUROC=0.9706 AUPRC=0.8503 meanConf=0.6778 ECE=0.1803\n",
      "  MESA  : loss=1.3696 acc=0.7821 macroF1=0.6901 kappa=0.6897 AUROC=0.9265 AUPRC=0.7579 meanConf=0.6220 ECE=0.1601\n",
      "  MESA F1/class: {'W': 0.8656622886306022, 'N1': 0.3784378581102198, 'N2': 0.7989929437436135, 'N3': 0.600760994206913, 'REM': 0.8065736140792611}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7871)\n",
      "  ? Added to VAL Top-K: VALBEST_ep010_valF1_0.7871.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 | train_loss=0.9887\n",
      "  VAL   : loss=0.8902 acc=0.8461 macroF1=0.7913 kappa=0.7879 AUROC=0.9677 AUPRC=0.8409 meanConf=0.6915 ECE=0.1546\n",
      "  F1/class: {'W': 0.9097645902117959, 'N1': 0.5142961430831001, 'N2': 0.8537726789369884, 'N3': 0.7999269639841148, 'REM': 0.8787435815541852}\n",
      "  TEST1 : loss=0.8545 acc=0.8554 macroF1=0.7998 kappa=0.8007 AUROC=0.9710 AUPRC=0.8522 meanConf=0.6916 ECE=0.1638\n",
      "  SHHS2 : loss=0.8246 acc=0.8628 macroF1=0.7952 kappa=0.8101 AUROC=0.9718 AUPRC=0.8531 meanConf=0.6817 ECE=0.1811\n",
      "  MESA  : loss=1.4022 acc=0.7698 macroF1=0.6713 kappa=0.6714 AUROC=0.9252 AUPRC=0.7506 meanConf=0.6238 ECE=0.1463\n",
      "  MESA F1/class: {'W': 0.8481995254822331, 'N1': 0.38374548385938695, 'N2': 0.7936095048748197, 'N3': 0.5308490249155322, 'REM': 0.799968129311853}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7913)\n",
      "  ? Added to VAL Top-K: VALBEST_ep011_valF1_0.7913.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 | train_loss=0.9710\n",
      "  VAL   : loss=0.8915 acc=0.8456 macroF1=0.7911 kappa=0.7875 AUROC=0.9680 AUPRC=0.8404 meanConf=0.6934 ECE=0.1522\n",
      "  F1/class: {'W': 0.9095300666351988, 'N1': 0.514005115089514, 'N2': 0.8517251527387021, 'N3': 0.8009426815924585, 'REM': 0.8793401278286406}\n",
      "  TEST1 : loss=0.8516 acc=0.8558 macroF1=0.8007 kappa=0.8015 AUROC=0.9713 AUPRC=0.8523 meanConf=0.6933 ECE=0.1626\n",
      "  SHHS2 : loss=0.8234 acc=0.8610 macroF1=0.7941 kappa=0.8079 AUROC=0.9717 AUPRC=0.8533 meanConf=0.6832 ECE=0.1778\n",
      "  MESA  : loss=1.3752 acc=0.7748 macroF1=0.6803 kappa=0.6785 AUROC=0.9264 AUPRC=0.7514 meanConf=0.6239 ECE=0.1511\n",
      "  MESA F1/class: {'W': 0.8568034782743202, 'N1': 0.3852569240654748, 'N2': 0.7910409705942075, 'N3': 0.5577179299091304, 'REM': 0.810677574624892}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 | train_loss=0.9684\n",
      "  VAL   : loss=0.8850 acc=0.8449 macroF1=0.7908 kappa=0.7869 AUROC=0.9679 AUPRC=0.8418 meanConf=0.6967 ECE=0.1482\n",
      "  F1/class: {'W': 0.9082074738931563, 'N1': 0.5136625119846597, 'N2': 0.8526589750722562, 'N3': 0.8028680294526543, 'REM': 0.876402712724434}\n",
      "  TEST1 : loss=0.8445 acc=0.8545 macroF1=0.7995 kappa=0.7999 AUROC=0.9713 AUPRC=0.8536 meanConf=0.6967 ECE=0.1577\n",
      "  SHHS2 : loss=0.8195 acc=0.8585 macroF1=0.7916 kappa=0.8048 AUROC=0.9715 AUPRC=0.8539 meanConf=0.6872 ECE=0.1713\n",
      "  MESA  : loss=1.4077 acc=0.7650 macroF1=0.6747 kappa=0.6655 AUROC=0.9226 AUPRC=0.7460 meanConf=0.6374 ECE=0.1279\n",
      "  MESA F1/class: {'W': 0.8497563893677142, 'N1': 0.39343182429971657, 'N2': 0.7789371372374208, 'N3': 0.5603705243156581, 'REM': 0.7911653693915682}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 | train_loss=0.9548\n",
      "  VAL   : loss=0.8771 acc=0.8468 macroF1=0.7922 kappa=0.7893 AUROC=0.9683 AUPRC=0.8433 meanConf=0.6931 ECE=0.1537\n",
      "  F1/class: {'W': 0.9068939710387078, 'N1': 0.5125799235900785, 'N2': 0.8563243874800285, 'N3': 0.8069317208626372, 'REM': 0.8782787233784063}\n",
      "  TEST1 : loss=0.8427 acc=0.8546 macroF1=0.7991 kappa=0.8000 AUROC=0.9712 AUPRC=0.8537 meanConf=0.6931 ECE=0.1616\n",
      "  SHHS2 : loss=0.8160 acc=0.8595 macroF1=0.7920 kappa=0.8061 AUROC=0.9717 AUPRC=0.8539 meanConf=0.6838 ECE=0.1757\n",
      "  MESA  : loss=1.4321 acc=0.7553 macroF1=0.6714 kappa=0.6523 AUROC=0.9206 AUPRC=0.7402 meanConf=0.6246 ECE=0.1307\n",
      "  MESA F1/class: {'W': 0.8319930991389327, 'N1': 0.3885050534587185, 'N2': 0.77042741581311, 'N3': 0.565725846623087, 'REM': 0.8001431690961247}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7922)\n",
      "  ? Added to VAL Top-K: VALBEST_ep014_valF1_0.7922.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 | train_loss=0.9511\n",
      "  VAL   : loss=0.8823 acc=0.8448 macroF1=0.7903 kappa=0.7871 AUROC=0.9685 AUPRC=0.8448 meanConf=0.7010 ECE=0.1439\n",
      "  F1/class: {'W': 0.9075792846993379, 'N1': 0.511520261194801, 'N2': 0.851831381488589, 'N3': 0.8003401635019459, 'REM': 0.8803279091490095}\n",
      "  TEST1 : loss=0.8479 acc=0.8512 macroF1=0.7961 kappa=0.7960 AUROC=0.9712 AUPRC=0.8548 meanConf=0.7001 ECE=0.1511\n",
      "  SHHS2 : loss=0.8132 acc=0.8578 macroF1=0.7892 kappa=0.8042 AUROC=0.9718 AUPRC=0.8538 meanConf=0.6947 ECE=0.1632\n",
      "  MESA  : loss=1.3294 acc=0.7929 macroF1=0.6934 kappa=0.7043 AUROC=0.9269 AUPRC=0.7491 meanConf=0.6514 ECE=0.1416\n",
      "  MESA F1/class: {'W': 0.8935188394934496, 'N1': 0.38956907876948726, 'N2': 0.800517561656673, 'N3': 0.5755478073977041, 'REM': 0.8078792467147848}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 | train_loss=0.9436\n",
      "  VAL   : loss=0.8772 acc=0.8511 macroF1=0.7954 kappa=0.7943 AUROC=0.9687 AUPRC=0.8441 meanConf=0.6948 ECE=0.1563\n",
      "  F1/class: {'W': 0.9099404790206919, 'N1': 0.515363967789752, 'N2': 0.8618366546531354, 'N3': 0.8096582377319959, 'REM': 0.880199371758525}\n",
      "  TEST1 : loss=0.8415 acc=0.8597 macroF1=0.8032 kappa=0.8059 AUROC=0.9718 AUPRC=0.8558 meanConf=0.6946 ECE=0.1650\n",
      "  SHHS2 : loss=0.8094 acc=0.8668 macroF1=0.7987 kappa=0.8154 AUROC=0.9726 AUPRC=0.8566 meanConf=0.6862 ECE=0.1806\n",
      "  MESA  : loss=1.4031 acc=0.7742 macroF1=0.6756 kappa=0.6769 AUROC=0.9241 AUPRC=0.7497 meanConf=0.6396 ECE=0.1358\n",
      "  MESA F1/class: {'W': 0.8579381094346009, 'N1': 0.3980831595837401, 'N2': 0.7919238107359045, 'N3': 0.5197893455791903, 'REM': 0.8101628773838911}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7954)\n",
      "  ? Added to VAL Top-K: VALBEST_ep016_valF1_0.7954.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 | train_loss=0.9326\n",
      "  VAL   : loss=0.8700 acc=0.8492 macroF1=0.7945 kappa=0.7925 AUROC=0.9691 AUPRC=0.8454 meanConf=0.6969 ECE=0.1522\n",
      "  F1/class: {'W': 0.9094795627685544, 'N1': 0.5147981707679211, 'N2': 0.8586646876607258, 'N3': 0.8101974215645094, 'REM': 0.8795101202698737}\n",
      "  TEST1 : loss=0.8332 acc=0.8582 macroF1=0.8031 kappa=0.8048 AUROC=0.9722 AUPRC=0.8570 meanConf=0.6969 ECE=0.1613\n",
      "  SHHS2 : loss=0.8121 acc=0.8622 macroF1=0.7950 kappa=0.8098 AUROC=0.9723 AUPRC=0.8564 meanConf=0.6884 ECE=0.1738\n",
      "  MESA  : loss=1.4625 acc=0.7584 macroF1=0.6555 kappa=0.6561 AUROC=0.9187 AUPRC=0.7312 meanConf=0.6391 ECE=0.1194\n",
      "  MESA F1/class: {'W': 0.8388488625653173, 'N1': 0.39340952873329765, 'N2': 0.7856442477574462, 'N3': 0.4610390655167976, 'REM': 0.7983643314290623}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 | train_loss=0.9305\n",
      "  VAL   : loss=0.8665 acc=0.8508 macroF1=0.7960 kappa=0.7945 AUROC=0.9691 AUPRC=0.8456 meanConf=0.6914 ECE=0.1594\n",
      "  F1/class: {'W': 0.9104029332605219, 'N1': 0.5158613094184514, 'N2': 0.8605584405246326, 'N3': 0.8146361658923184, 'REM': 0.8786612914013997}\n",
      "  TEST1 : loss=0.8318 acc=0.8598 macroF1=0.8047 kappa=0.8068 AUROC=0.9722 AUPRC=0.8573 meanConf=0.6913 ECE=0.1685\n",
      "  SHHS2 : loss=0.8080 acc=0.8651 macroF1=0.7974 kappa=0.8136 AUROC=0.9726 AUPRC=0.8568 meanConf=0.6815 ECE=0.1836\n",
      "  MESA  : loss=1.4099 acc=0.7656 macroF1=0.6651 kappa=0.6651 AUROC=0.9228 AUPRC=0.7400 meanConf=0.6287 ECE=0.1378\n",
      "  MESA F1/class: {'W': 0.853673146732744, 'N1': 0.3964539117204117, 'N2': 0.781464934521073, 'N3': 0.49709213877756814, 'REM': 0.7967603796900727}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7960)\n",
      "  ? Added to VAL Top-K: VALBEST_ep018_valF1_0.7960.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 | train_loss=0.9251\n",
      "  VAL   : loss=0.8666 acc=0.8528 macroF1=0.7973 kappa=0.7968 AUROC=0.9693 AUPRC=0.8470 meanConf=0.6982 ECE=0.1546\n",
      "  F1/class: {'W': 0.9107019891641872, 'N1': 0.5168215575666572, 'N2': 0.8629085397229719, 'N3': 0.8159862404894604, 'REM': 0.8799265268636155}\n",
      "  TEST1 : loss=0.8285 acc=0.8625 macroF1=0.8066 kappa=0.8101 AUROC=0.9726 AUPRC=0.8594 meanConf=0.6979 ECE=0.1647\n",
      "  SHHS2 : loss=0.7974 acc=0.8682 macroF1=0.8005 kappa=0.8175 AUROC=0.9729 AUPRC=0.8586 meanConf=0.6897 ECE=0.1785\n",
      "  MESA  : loss=1.3447 acc=0.7842 macroF1=0.6767 kappa=0.6903 AUROC=0.9272 AUPRC=0.7482 meanConf=0.6375 ECE=0.1467\n",
      "  MESA F1/class: {'W': 0.8833401487675528, 'N1': 0.4026656453466105, 'N2': 0.7930738729742183, 'N3': 0.49032049057040383, 'REM': 0.8139613547736931}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7973)\n",
      "  ? Added to VAL Top-K: VALBEST_ep019_valF1_0.7973.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 | train_loss=0.9178\n",
      "  VAL   : loss=0.8624 acc=0.8497 macroF1=0.7955 kappa=0.7935 AUROC=0.9694 AUPRC=0.8472 meanConf=0.6961 ECE=0.1536\n",
      "  F1/class: {'W': 0.910900980336798, 'N1': 0.5176584421867442, 'N2': 0.8575299782834482, 'N3': 0.8135580252108877, 'REM': 0.8780830717393675}\n",
      "  TEST1 : loss=0.8283 acc=0.8569 macroF1=0.8022 kappa=0.8034 AUROC=0.9722 AUPRC=0.8588 meanConf=0.6960 ECE=0.1609\n",
      "  SHHS2 : loss=0.8036 acc=0.8624 macroF1=0.7956 kappa=0.8102 AUROC=0.9724 AUPRC=0.8576 meanConf=0.6866 ECE=0.1757\n",
      "  MESA  : loss=1.3337 acc=0.7816 macroF1=0.6836 kappa=0.6886 AUROC=0.9278 AUPRC=0.7489 meanConf=0.6225 ECE=0.1591\n",
      "  MESA F1/class: {'W': 0.8799828051918054, 'N1': 0.39132500784202895, 'N2': 0.7905464350860497, 'N3': 0.5545614488572406, 'REM': 0.8013662754491836}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 | train_loss=0.9127\n",
      "  VAL   : loss=0.8611 acc=0.8535 macroF1=0.7981 kappa=0.7980 AUROC=0.9698 AUPRC=0.8476 meanConf=0.6954 ECE=0.1581\n",
      "  F1/class: {'W': 0.9116560440747542, 'N1': 0.5175024422012373, 'N2': 0.8631839517993508, 'N3': 0.817486810928275, 'REM': 0.8806242634802199}\n",
      "  TEST1 : loss=0.8270 acc=0.8614 macroF1=0.8056 kappa=0.8087 AUROC=0.9727 AUPRC=0.8601 meanConf=0.6956 ECE=0.1659\n",
      "  SHHS2 : loss=0.8049 acc=0.8666 macroF1=0.7985 kappa=0.8153 AUROC=0.9727 AUPRC=0.8583 meanConf=0.6871 ECE=0.1795\n",
      "  MESA  : loss=1.3670 acc=0.7763 macroF1=0.6633 kappa=0.6788 AUROC=0.9248 AUPRC=0.7428 meanConf=0.6340 ECE=0.1423\n",
      "  MESA F1/class: {'W': 0.8760865181131026, 'N1': 0.4073827685680448, 'N2': 0.7897653854742318, 'N3': 0.450267798306504, 'REM': 0.792865100483342}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7981)\n",
      "  ? Added to VAL Top-K: VALBEST_ep021_valF1_0.7981.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 | train_loss=0.9052\n",
      "  VAL   : loss=0.8611 acc=0.8461 macroF1=0.7937 kappa=0.7895 AUROC=0.9698 AUPRC=0.8481 meanConf=0.6966 ECE=0.1496\n",
      "  F1/class: {'W': 0.913212066900468, 'N1': 0.5214028591998354, 'N2': 0.8499642162154398, 'N3': 0.8030098776930807, 'REM': 0.8807862023444281}\n",
      "  TEST1 : loss=0.8261 acc=0.8542 macroF1=0.8015 kappa=0.8003 AUROC=0.9729 AUPRC=0.8607 meanConf=0.6966 ECE=0.1576\n",
      "  SHHS2 : loss=0.7994 acc=0.8597 macroF1=0.7931 kappa=0.8071 AUROC=0.9731 AUPRC=0.8590 meanConf=0.6909 ECE=0.1687\n",
      "  MESA  : loss=1.3383 acc=0.7861 macroF1=0.6831 kappa=0.6938 AUROC=0.9280 AUPRC=0.7462 meanConf=0.6327 ECE=0.1534\n",
      "  MESA F1/class: {'W': 0.8847800344959317, 'N1': 0.3908507768299902, 'N2': 0.7937514948993454, 'N3': 0.5324008904311389, 'REM': 0.8137028875446652}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 | train_loss=0.8997\n",
      "  VAL   : loss=0.8602 acc=0.8544 macroF1=0.7995 kappa=0.7992 AUROC=0.9699 AUPRC=0.8474 meanConf=0.6988 ECE=0.1557\n",
      "  F1/class: {'W': 0.9136424310232364, 'N1': 0.5192790867905117, 'N2': 0.8641220733447089, 'N3': 0.819923777260216, 'REM': 0.8807334951337303}\n",
      "  TEST1 : loss=0.8235 acc=0.8634 macroF1=0.8083 kappa=0.8114 AUROC=0.9731 AUPRC=0.8598 meanConf=0.6990 ECE=0.1644\n",
      "  SHHS2 : loss=0.7894 acc=0.8702 macroF1=0.8033 kappa=0.8202 AUROC=0.9737 AUPRC=0.8611 meanConf=0.6931 ECE=0.1770\n",
      "  MESA  : loss=1.3369 acc=0.7911 macroF1=0.6718 kappa=0.6991 AUROC=0.9290 AUPRC=0.7519 meanConf=0.6493 ECE=0.1418\n",
      "  MESA F1/class: {'W': 0.8936444929991583, 'N1': 0.40481014915644054, 'N2': 0.8008336133987937, 'N3': 0.44194906356555047, 'REM': 0.817597660300688}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.7995)\n",
      "  ? Added to VAL Top-K: VALBEST_ep023_valF1_0.7995.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 | train_loss=0.9016\n",
      "  VAL   : loss=0.8577 acc=0.8529 macroF1=0.7986 kappa=0.7976 AUROC=0.9698 AUPRC=0.8481 meanConf=0.6991 ECE=0.1538\n",
      "  F1/class: {'W': 0.9120627882151084, 'N1': 0.5194659620889129, 'N2': 0.8613799548248642, 'N3': 0.819158181464427, 'REM': 0.8807184956497335}\n",
      "  TEST1 : loss=0.8203 acc=0.8619 macroF1=0.8071 kappa=0.8098 AUROC=0.9730 AUPRC=0.8602 meanConf=0.6993 ECE=0.1626\n",
      "  SHHS2 : loss=0.7864 acc=0.8684 macroF1=0.8012 kappa=0.8180 AUROC=0.9734 AUPRC=0.8599 meanConf=0.6931 ECE=0.1752\n",
      "  MESA  : loss=1.3721 acc=0.7792 macroF1=0.6615 kappa=0.6829 AUROC=0.9257 AUPRC=0.7402 meanConf=0.6401 ECE=0.1391\n",
      "  MESA F1/class: {'W': 0.8763674009317307, 'N1': 0.39752121868185314, 'N2': 0.7944247125792737, 'N3': 0.4285353488111051, 'REM': 0.810701112786348}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 | train_loss=0.8899\n",
      "  VAL   : loss=0.8599 acc=0.8515 macroF1=0.7976 kappa=0.7960 AUROC=0.9698 AUPRC=0.8461 meanConf=0.6973 ECE=0.1542\n",
      "  F1/class: {'W': 0.9130998813679386, 'N1': 0.5167317291071879, 'N2': 0.8586733487778356, 'N3': 0.8206647568967546, 'REM': 0.8787206338144407}\n",
      "  TEST1 : loss=0.8208 acc=0.8617 macroF1=0.8078 kappa=0.8099 AUROC=0.9732 AUPRC=0.8591 meanConf=0.6976 ECE=0.1641\n",
      "  SHHS2 : loss=0.7843 acc=0.8681 macroF1=0.8023 kappa=0.8179 AUROC=0.9738 AUPRC=0.8608 meanConf=0.6911 ECE=0.1770\n",
      "  MESA  : loss=1.3308 acc=0.7812 macroF1=0.6605 kappa=0.6871 AUROC=0.9267 AUPRC=0.7378 meanConf=0.6382 ECE=0.1430\n",
      "  MESA F1/class: {'W': 0.8883402062900219, 'N1': 0.4115315488317526, 'N2': 0.7947059671223056, 'N3': 0.41309862255788804, 'REM': 0.7947837143690692}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 | train_loss=0.8836\n",
      "  VAL   : loss=0.8583 acc=0.8547 macroF1=0.7992 kappa=0.7995 AUROC=0.9697 AUPRC=0.8480 meanConf=0.6995 ECE=0.1551\n",
      "  F1/class: {'W': 0.912434036939314, 'N1': 0.5187906188188753, 'N2': 0.865178615101169, 'N3': 0.8210889205896338, 'REM': 0.8783786246742122}\n",
      "  TEST1 : loss=0.8197 acc=0.8648 macroF1=0.8091 kappa=0.8131 AUROC=0.9731 AUPRC=0.8603 meanConf=0.6998 ECE=0.1650\n",
      "  SHHS2 : loss=0.7970 acc=0.8688 macroF1=0.8014 kappa=0.8184 AUROC=0.9728 AUPRC=0.8596 meanConf=0.6927 ECE=0.1762\n",
      "  MESA  : loss=1.3723 acc=0.7843 macroF1=0.6643 kappa=0.6900 AUROC=0.9251 AUPRC=0.7373 meanConf=0.6505 ECE=0.1339\n",
      "  MESA F1/class: {'W': 0.8867353658885776, 'N1': 0.3971951792142745, 'N2': 0.7966779071892183, 'N3': 0.4265058654198926, 'REM': 0.8146156401335688}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 | train_loss=0.8793\n",
      "  VAL   : loss=0.8578 acc=0.8531 macroF1=0.7983 kappa=0.7978 AUROC=0.9696 AUPRC=0.8481 meanConf=0.6995 ECE=0.1536\n",
      "  F1/class: {'W': 0.911131573918029, 'N1': 0.5176517812343201, 'N2': 0.8625515715997926, 'N3': 0.8204332764593352, 'REM': 0.8796560208531267}\n",
      "  TEST1 : loss=0.8183 acc=0.8624 macroF1=0.8074 kappa=0.8104 AUROC=0.9730 AUPRC=0.8601 meanConf=0.6995 ECE=0.1629\n",
      "  SHHS2 : loss=0.7918 acc=0.8663 macroF1=0.7988 kappa=0.8155 AUROC=0.9726 AUPRC=0.8590 meanConf=0.6929 ECE=0.1734\n",
      "  MESA  : loss=1.3769 acc=0.7812 macroF1=0.6484 kappa=0.6851 AUROC=0.9226 AUPRC=0.7285 meanConf=0.6505 ECE=0.1307\n",
      "  MESA F1/class: {'W': 0.8895659762743169, 'N1': 0.398053187506521, 'N2': 0.7942514651436017, 'N3': 0.3561363718931487, 'REM': 0.8039592258827005}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 | train_loss=0.8785\n",
      "  VAL   : loss=0.8535 acc=0.8533 macroF1=0.7990 kappa=0.7984 AUROC=0.9701 AUPRC=0.8486 meanConf=0.6991 ECE=0.1542\n",
      "  F1/class: {'W': 0.9132907659121927, 'N1': 0.5193561897923772, 'N2': 0.8616577685064055, 'N3': 0.820744516527649, 'REM': 0.8797908234184695}\n",
      "  TEST1 : loss=0.8153 acc=0.8628 macroF1=0.8086 kappa=0.8111 AUROC=0.9734 AUPRC=0.8616 meanConf=0.6991 ECE=0.1637\n",
      "  SHHS2 : loss=0.7843 acc=0.8679 macroF1=0.8008 kappa=0.8178 AUROC=0.9734 AUPRC=0.8610 meanConf=0.6937 ECE=0.1742\n",
      "  MESA  : loss=1.3423 acc=0.7885 macroF1=0.6596 kappa=0.6954 AUROC=0.9265 AUPRC=0.7372 meanConf=0.6494 ECE=0.1391\n",
      "  MESA F1/class: {'W': 0.8970256294823458, 'N1': 0.4101891605439488, 'N2': 0.7978533902597873, 'N3': 0.3749288574769499, 'REM': 0.8180813622528917}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 | train_loss=0.8712\n",
      "  VAL   : loss=0.8562 acc=0.8568 macroF1=0.8019 kappa=0.8025 AUROC=0.9702 AUPRC=0.8488 meanConf=0.6985 ECE=0.1583\n",
      "  F1/class: {'W': 0.9147962818323049, 'N1': 0.5222609682912983, 'N2': 0.8653339068314851, 'N3': 0.8254334653621173, 'REM': 0.8819111537232341}\n",
      "  TEST1 : loss=0.8165 acc=0.8655 macroF1=0.8108 kappa=0.8144 AUROC=0.9737 AUPRC=0.8622 meanConf=0.6987 ECE=0.1668\n",
      "  SHHS2 : loss=0.7900 acc=0.8695 macroF1=0.8029 kappa=0.8194 AUROC=0.9739 AUPRC=0.8616 meanConf=0.6927 ECE=0.1767\n",
      "  MESA  : loss=1.4269 acc=0.7713 macroF1=0.6292 kappa=0.6698 AUROC=0.9210 AUPRC=0.7162 meanConf=0.6540 ECE=0.1173\n",
      "  MESA F1/class: {'W': 0.8768155814387883, 'N1': 0.4071434383372159, 'N2': 0.7838374053621394, 'N3': 0.2764576584914391, 'REM': 0.801973383872619}\n",
      "  ? Saved BEST_VAL: BEST_VAL_macroF1.pt (val_macroF1=0.8019)\n",
      "  ? Added to VAL Top-K: VALBEST_ep029_valF1_0.8019.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 | train_loss=0.8682\n",
      "  VAL   : loss=0.8568 acc=0.8536 macroF1=0.7993 kappa=0.7985 AUROC=0.9702 AUPRC=0.8491 meanConf=0.7001 ECE=0.1535\n",
      "  F1/class: {'W': 0.9123386305585208, 'N1': 0.5201574044330228, 'N2': 0.8621761463410054, 'N3': 0.8211059605629455, 'REM': 0.880825893406134}\n",
      "  TEST1 : loss=0.8173 acc=0.8632 macroF1=0.8087 kappa=0.8115 AUROC=0.9736 AUPRC=0.8622 meanConf=0.7004 ECE=0.1628\n",
      "  SHHS2 : loss=0.8010 acc=0.8645 macroF1=0.7981 kappa=0.8130 AUROC=0.9730 AUPRC=0.8602 meanConf=0.6942 ECE=0.1704\n",
      "  MESA  : loss=1.3991 acc=0.7746 macroF1=0.6423 kappa=0.6755 AUROC=0.9220 AUPRC=0.7256 meanConf=0.6501 ECE=0.1245\n",
      "  MESA F1/class: {'W': 0.8796484356341056, 'N1': 0.4026582591862362, 'N2': 0.787439690668735, 'N3': 0.33545113515623304, 'REM': 0.8061734972772122}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 | train_loss=0.8636\n",
      "  VAL   : loss=0.8580 acc=0.8544 macroF1=0.7997 kappa=0.7995 AUROC=0.9700 AUPRC=0.8481 meanConf=0.6984 ECE=0.1561\n",
      "  F1/class: {'W': 0.9136555325563218, 'N1': 0.5190575747828885, 'N2': 0.8627768127764611, 'N3': 0.8236321797119966, 'REM': 0.8792940279979843}\n",
      "  TEST1 : loss=0.8156 acc=0.8654 macroF1=0.8107 kappa=0.8143 AUROC=0.9738 AUPRC=0.8621 meanConf=0.6988 ECE=0.1666\n",
      "  SHHS2 : loss=0.8014 acc=0.8675 macroF1=0.8012 kappa=0.8167 AUROC=0.9731 AUPRC=0.8605 meanConf=0.6916 ECE=0.1759\n",
      "  MESA  : loss=1.3953 acc=0.7733 macroF1=0.6382 kappa=0.6742 AUROC=0.9214 AUPRC=0.7154 meanConf=0.6461 ECE=0.1273\n",
      "  MESA F1/class: {'W': 0.882502743958533, 'N1': 0.4056757427148053, 'N2': 0.7867357071516771, 'N3': 0.3155043796948509, 'REM': 0.800520412957319}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 | train_loss=0.8562\n",
      "  VAL   : loss=0.8550 acc=0.8563 macroF1=0.8016 kappa=0.8019 AUROC=0.9701 AUPRC=0.8491 meanConf=0.7016 ECE=0.1547\n",
      "  F1/class: {'W': 0.9131450743694762, 'N1': 0.5227945172392972, 'N2': 0.8657903281947537, 'N3': 0.8255928270871838, 'REM': 0.8808463661453543}\n",
      "  TEST1 : loss=0.8154 acc=0.8656 macroF1=0.8107 kappa=0.8144 AUROC=0.9736 AUPRC=0.8628 meanConf=0.7015 ECE=0.1640\n",
      "  SHHS2 : loss=0.7958 acc=0.8685 macroF1=0.8014 kappa=0.8180 AUROC=0.9729 AUPRC=0.8605 meanConf=0.6958 ECE=0.1726\n",
      "  MESA  : loss=1.4175 acc=0.7754 macroF1=0.6429 kappa=0.6768 AUROC=0.9214 AUPRC=0.7214 meanConf=0.6581 ECE=0.1173\n",
      "  MESA F1/class: {'W': 0.8786775860722529, 'N1': 0.40466203904288695, 'N2': 0.7912759492232982, 'N3': 0.33109626480412996, 'REM': 0.8087639641142462}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 | train_loss=0.8496\n",
      "  VAL   : loss=0.8557 acc=0.8544 macroF1=0.8000 kappa=0.7997 AUROC=0.9698 AUPRC=0.8491 meanConf=0.7022 ECE=0.1522\n",
      "  F1/class: {'W': 0.9123773425257187, 'N1': 0.5201784828953891, 'N2': 0.8639929372594783, 'N3': 0.82425782693542, 'REM': 0.8790583619421285}\n",
      "  TEST1 : loss=0.8137 acc=0.8639 macroF1=0.8095 kappa=0.8124 AUROC=0.9736 AUPRC=0.8629 meanConf=0.7022 ECE=0.1617\n",
      "  SHHS2 : loss=0.7865 acc=0.8690 macroF1=0.8015 kappa=0.8191 AUROC=0.9731 AUPRC=0.8600 meanConf=0.6978 ECE=0.1713\n",
      "  MESA  : loss=1.4026 acc=0.7752 macroF1=0.6430 kappa=0.6767 AUROC=0.9197 AUPRC=0.7134 meanConf=0.6538 ECE=0.1214\n",
      "  MESA F1/class: {'W': 0.8846283901444393, 'N1': 0.39455586338784415, 'N2': 0.7870548719153613, 'N3': 0.3331700052841428, 'REM': 0.8153486608284607}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 | train_loss=0.8420\n",
      "  VAL   : loss=0.8575 acc=0.8539 macroF1=0.7999 kappa=0.7989 AUROC=0.9698 AUPRC=0.8486 meanConf=0.7024 ECE=0.1515\n",
      "  F1/class: {'W': 0.9125381894021986, 'N1': 0.5219168568310348, 'N2': 0.8627161015457201, 'N3': 0.8223946971511428, 'REM': 0.8800364608353964}\n",
      "  TEST1 : loss=0.8154 acc=0.8640 macroF1=0.8095 kappa=0.8125 AUROC=0.9736 AUPRC=0.8622 meanConf=0.7025 ECE=0.1615\n",
      "  SHHS2 : loss=0.7938 acc=0.8669 macroF1=0.8002 kappa=0.8161 AUROC=0.9728 AUPRC=0.8596 meanConf=0.6978 ECE=0.1690\n",
      "  MESA  : loss=1.4791 acc=0.7571 macroF1=0.6337 kappa=0.6521 AUROC=0.9150 AUPRC=0.7043 meanConf=0.6440 ECE=0.1131\n",
      "  MESA F1/class: {'W': 0.8546955824309515, 'N1': 0.3873440350683858, 'N2': 0.7778039337014593, 'N3': 0.3454018234540182, 'REM': 0.8033275698725117}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 | train_loss=0.8402\n",
      "  VAL   : loss=0.8563 acc=0.8520 macroF1=0.7982 kappa=0.7968 AUROC=0.9698 AUPRC=0.8491 meanConf=0.7016 ECE=0.1504\n",
      "  F1/class: {'W': 0.9125238222536499, 'N1': 0.5195382235093426, 'N2': 0.8597564726871767, 'N3': 0.819664838551828, 'REM': 0.8797045525652274}\n",
      "  TEST1 : loss=0.8150 acc=0.8616 macroF1=0.8078 kappa=0.8098 AUROC=0.9733 AUPRC=0.8626 meanConf=0.7015 ECE=0.1602\n",
      "  SHHS2 : loss=0.7888 acc=0.8653 macroF1=0.7978 kappa=0.8144 AUROC=0.9729 AUPRC=0.8585 meanConf=0.6971 ECE=0.1682\n",
      "  MESA  : loss=1.4271 acc=0.7646 macroF1=0.6339 kappa=0.6625 AUROC=0.9171 AUPRC=0.7062 meanConf=0.6450 ECE=0.1195\n",
      "  MESA F1/class: {'W': 0.8721942870746592, 'N1': 0.39020873160472336, 'N2': 0.7805933173059852, 'N3': 0.32897586482742475, 'REM': 0.7974655692842949}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 | train_loss=0.8331\n",
      "  VAL   : loss=0.8586 acc=0.8553 macroF1=0.8006 kappa=0.8008 AUROC=0.9698 AUPRC=0.8475 meanConf=0.7031 ECE=0.1522\n",
      "  F1/class: {'W': 0.9138710740124331, 'N1': 0.5203177391026664, 'N2': 0.864138593717329, 'N3': 0.8243046150640693, 'REM': 0.8803278688524591}\n",
      "  TEST1 : loss=0.8158 acc=0.8658 macroF1=0.8111 kappa=0.8149 AUROC=0.9734 AUPRC=0.8621 meanConf=0.7032 ECE=0.1626\n",
      "  SHHS2 : loss=0.7810 acc=0.8708 macroF1=0.8035 kappa=0.8214 AUROC=0.9734 AUPRC=0.8606 meanConf=0.6991 ECE=0.1717\n",
      "  MESA  : loss=1.4345 acc=0.7691 macroF1=0.6297 kappa=0.6683 AUROC=0.9153 AUPRC=0.6965 meanConf=0.6538 ECE=0.1152\n",
      "  MESA F1/class: {'W': 0.8819118698339179, 'N1': 0.3942627736347604, 'N2': 0.7826534440303646, 'N3': 0.2889515555398609, 'REM': 0.8007771148316682}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 | train_loss=0.8296\n",
      "  VAL   : loss=0.8585 acc=0.8546 macroF1=0.8002 kappa=0.8001 AUROC=0.9696 AUPRC=0.8469 meanConf=0.7052 ECE=0.1494\n",
      "  F1/class: {'W': 0.9148158005443134, 'N1': 0.5211702707614029, 'N2': 0.8626023229306982, 'N3': 0.8221763643259853, 'REM': 0.8802503442848711}\n",
      "  TEST1 : loss=0.8151 acc=0.8648 macroF1=0.8104 kappa=0.8137 AUROC=0.9733 AUPRC=0.8621 meanConf=0.7057 ECE=0.1591\n",
      "  SHHS2 : loss=0.7859 acc=0.8692 macroF1=0.8019 kappa=0.8191 AUROC=0.9728 AUPRC=0.8603 meanConf=0.7025 ECE=0.1666\n",
      "  MESA  : loss=1.4441 acc=0.7680 macroF1=0.6341 kappa=0.6669 AUROC=0.9146 AUPRC=0.6994 meanConf=0.6576 ECE=0.1104\n",
      "  MESA F1/class: {'W': 0.8767190993155212, 'N1': 0.38940561279742564, 'N2': 0.7843447502916584, 'N3': 0.3153964541612227, 'REM': 0.8046285643454594}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 | train_loss=0.8243\n",
      "  VAL   : loss=0.8594 acc=0.8538 macroF1=0.7996 kappa=0.7990 AUROC=0.9695 AUPRC=0.8471 meanConf=0.7056 ECE=0.1482\n",
      "  F1/class: {'W': 0.9144481223408594, 'N1': 0.5204850340410103, 'N2': 0.8611620506776664, 'N3': 0.8220400459649101, 'REM': 0.8797438796578507}\n",
      "  TEST1 : loss=0.8165 acc=0.8641 macroF1=0.8098 kappa=0.8129 AUROC=0.9731 AUPRC=0.8620 meanConf=0.7061 ECE=0.1580\n",
      "  SHHS2 : loss=0.7890 acc=0.8676 macroF1=0.8005 kappa=0.8171 AUROC=0.9724 AUPRC=0.8590 meanConf=0.7025 ECE=0.1650\n",
      "  MESA  : loss=1.4246 acc=0.7775 macroF1=0.6401 kappa=0.6800 AUROC=0.9176 AUPRC=0.7021 meanConf=0.6690 ECE=0.1085\n",
      "  MESA F1/class: {'W': 0.8920700515305772, 'N1': 0.3974041609592196, 'N2': 0.7890439689396019, 'N3': 0.31111807911214995, 'REM': 0.8110217718155429}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 | train_loss=0.8199\n",
      "  VAL   : loss=0.8600 acc=0.8551 macroF1=0.8006 kappa=0.8007 AUROC=0.9696 AUPRC=0.8471 meanConf=0.7053 ECE=0.1498\n",
      "  F1/class: {'W': 0.9143165587823279, 'N1': 0.5193387771360287, 'N2': 0.8632714960852084, 'N3': 0.8257741713870921, 'REM': 0.880437800386222}\n",
      "  TEST1 : loss=0.8167 acc=0.8655 macroF1=0.8107 kappa=0.8146 AUROC=0.9732 AUPRC=0.8616 meanConf=0.7057 ECE=0.1598\n",
      "  SHHS2 : loss=0.7893 acc=0.8689 macroF1=0.8017 kappa=0.8187 AUROC=0.9723 AUPRC=0.8588 meanConf=0.7024 ECE=0.1665\n",
      "  MESA  : loss=1.4236 acc=0.7685 macroF1=0.6329 kappa=0.6690 AUROC=0.9152 AUPRC=0.6949 meanConf=0.6596 ECE=0.1089\n",
      "  MESA F1/class: {'W': 0.8854503464203234, 'N1': 0.39397409596129707, 'N2': 0.7840195306143909, 'N3': 0.30697378197643566, 'REM': 0.7942644613604278}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 | train_loss=0.8144\n",
      "  VAL   : loss=0.8603 acc=0.8543 macroF1=0.7997 kappa=0.7995 AUROC=0.9692 AUPRC=0.8470 meanConf=0.7068 ECE=0.1475\n",
      "  F1/class: {'W': 0.9134511173424624, 'N1': 0.517702097292154, 'N2': 0.8627991833032985, 'N3': 0.8252403285569921, 'REM': 0.8794703618167822}\n",
      "  TEST1 : loss=0.8168 acc=0.8657 macroF1=0.8111 kappa=0.8149 AUROC=0.9728 AUPRC=0.8615 meanConf=0.7070 ECE=0.1587\n",
      "  SHHS2 : loss=0.7930 acc=0.8678 macroF1=0.8002 kappa=0.8172 AUROC=0.9717 AUPRC=0.8569 meanConf=0.7040 ECE=0.1638\n",
      "  MESA  : loss=1.3988 acc=0.7791 macroF1=0.6486 kappa=0.6836 AUROC=0.9179 AUPRC=0.7104 meanConf=0.6678 ECE=0.1114\n",
      "  MESA F1/class: {'W': 0.8958440489661558, 'N1': 0.3891990598222052, 'N2': 0.7904344689934365, 'N3': 0.36991260467327314, 'REM': 0.7975294550430264}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41 | train_loss=0.8055\n",
      "  VAL   : loss=0.8637 acc=0.8570 macroF1=0.8015 kappa=0.8026 AUROC=0.9691 AUPRC=0.8465 meanConf=0.7099 ECE=0.1471\n",
      "  F1/class: {'W': 0.9142886665403758, 'N1': 0.5184127431065344, 'N2': 0.8667551148662803, 'N3': 0.8280248039876774, 'REM': 0.8799133223755086}\n",
      "  TEST1 : loss=0.8211 acc=0.8676 macroF1=0.8116 kappa=0.8169 AUROC=0.9727 AUPRC=0.8606 meanConf=0.7104 ECE=0.1573\n",
      "  SHHS2 : loss=0.7949 acc=0.8711 macroF1=0.8030 kappa=0.8213 AUROC=0.9716 AUPRC=0.8565 meanConf=0.7066 ECE=0.1645\n",
      "  MESA  : loss=1.4639 acc=0.7633 macroF1=0.6287 kappa=0.6613 AUROC=0.9113 AUPRC=0.6885 meanConf=0.6626 ECE=0.1007\n",
      "  MESA F1/class: {'W': 0.8770511102371925, 'N1': 0.38348326780040665, 'N2': 0.7795285938410402, 'N3': 0.3106751657691322, 'REM': 0.7925926433928038}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42 | train_loss=0.7986\n",
      "  VAL   : loss=0.8635 acc=0.8566 macroF1=0.8016 kappa=0.8022 AUROC=0.9691 AUPRC=0.8469 meanConf=0.7089 ECE=0.1477\n",
      "  F1/class: {'W': 0.9129112595542977, 'N1': 0.5193354683746997, 'N2': 0.8661584875020124, 'N3': 0.8288816603289503, 'REM': 0.8809109541117393}\n",
      "  TEST1 : loss=0.8195 acc=0.8668 macroF1=0.8113 kappa=0.8160 AUROC=0.9728 AUPRC=0.8611 meanConf=0.7093 ECE=0.1576\n",
      "  SHHS2 : loss=0.7959 acc=0.8703 macroF1=0.8019 kappa=0.8202 AUROC=0.9715 AUPRC=0.8560 meanConf=0.7066 ECE=0.1637\n",
      "  MESA  : loss=1.4604 acc=0.7687 macroF1=0.6334 kappa=0.6685 AUROC=0.9128 AUPRC=0.6906 meanConf=0.6729 ECE=0.0959\n",
      "  MESA F1/class: {'W': 0.8860039281656523, 'N1': 0.39464912051663664, 'N2': 0.7798451714874246, 'N3': 0.3024585449782757, 'REM': 0.8038458161383791}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43 | train_loss=0.7894\n",
      "  VAL   : loss=0.8669 acc=0.8564 macroF1=0.8008 kappa=0.8019 AUROC=0.9687 AUPRC=0.8452 meanConf=0.7097 ECE=0.1468\n",
      "  F1/class: {'W': 0.913268019834257, 'N1': 0.5157785411839898, 'N2': 0.86598798903854, 'N3': 0.8283748028794448, 'REM': 0.8807001495729138}\n",
      "  TEST1 : loss=0.8230 acc=0.8676 macroF1=0.8116 kappa=0.8168 AUROC=0.9724 AUPRC=0.8596 meanConf=0.7102 ECE=0.1573\n",
      "  SHHS2 : loss=0.7994 acc=0.8705 macroF1=0.8020 kappa=0.8204 AUROC=0.9709 AUPRC=0.8545 meanConf=0.7076 ECE=0.1629\n",
      "  MESA  : loss=1.4873 acc=0.7678 macroF1=0.6274 kappa=0.6668 AUROC=0.9100 AUPRC=0.6778 meanConf=0.6785 ECE=0.0893\n",
      "  MESA F1/class: {'W': 0.8856587859232609, 'N1': 0.38847153177237437, 'N2': 0.7809815219272196, 'N3': 0.278640667689659, 'REM': 0.8032762984783094}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44 | train_loss=0.7864\n",
      "  VAL   : loss=0.8679 acc=0.8570 macroF1=0.8014 kappa=0.8026 AUROC=0.9686 AUPRC=0.8456 meanConf=0.7135 ECE=0.1435\n",
      "  F1/class: {'W': 0.9135736644766261, 'N1': 0.5180412893292005, 'N2': 0.8666085724670773, 'N3': 0.8270657005020651, 'REM': 0.881616927441434}\n",
      "  TEST1 : loss=0.8241 acc=0.8673 macroF1=0.8112 kappa=0.8164 AUROC=0.9723 AUPRC=0.8598 meanConf=0.7137 ECE=0.1536\n",
      "  SHHS2 : loss=0.7995 acc=0.8706 macroF1=0.8015 kappa=0.8205 AUROC=0.9707 AUPRC=0.8542 meanConf=0.7117 ECE=0.1589\n",
      "  MESA  : loss=1.5138 acc=0.7642 macroF1=0.6212 kappa=0.6619 AUROC=0.9064 AUPRC=0.6716 meanConf=0.6807 ECE=0.0835\n",
      "  MESA F1/class: {'W': 0.8840860200126082, 'N1': 0.38231866858110913, 'N2': 0.7775108618861565, 'N3': 0.2676878566713638, 'REM': 0.7944912804493954}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45 | train_loss=0.7820\n",
      "  VAL   : loss=0.8705 acc=0.8549 macroF1=0.7997 kappa=0.8001 AUROC=0.9685 AUPRC=0.8443 meanConf=0.7130 ECE=0.1419\n",
      "  F1/class: {'W': 0.9131344260037187, 'N1': 0.5151141516188052, 'N2': 0.8643058807568423, 'N3': 0.8262639663896152, 'REM': 0.8794717247616627}\n",
      "  TEST1 : loss=0.8264 acc=0.8655 macroF1=0.8098 kappa=0.8143 AUROC=0.9723 AUPRC=0.8589 meanConf=0.7131 ECE=0.1524\n",
      "  SHHS2 : loss=0.8046 acc=0.8681 macroF1=0.7992 kappa=0.8173 AUROC=0.9707 AUPRC=0.8536 meanConf=0.7104 ECE=0.1577\n",
      "  MESA  : loss=1.5320 acc=0.7609 macroF1=0.6249 kappa=0.6584 AUROC=0.9046 AUPRC=0.6712 meanConf=0.6804 ECE=0.0806\n",
      "  MESA F1/class: {'W': 0.8780594124271174, 'N1': 0.3827318972105464, 'N2': 0.7773572144872781, 'N3': 0.28759407644693, 'REM': 0.7986190185241758}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46 | train_loss=0.7754\n",
      "  VAL   : loss=0.8751 acc=0.8562 macroF1=0.8001 kappa=0.8015 AUROC=0.9682 AUPRC=0.8436 meanConf=0.7141 ECE=0.1421\n",
      "  F1/class: {'W': 0.9136829136829137, 'N1': 0.5126742585907741, 'N2': 0.8661305427555712, 'N3': 0.8282278824455871, 'REM': 0.8795420112758613}\n",
      "  TEST1 : loss=0.8316 acc=0.8666 macroF1=0.8100 kappa=0.8154 AUROC=0.9720 AUPRC=0.8578 meanConf=0.7142 ECE=0.1524\n",
      "  SHHS2 : loss=0.8024 acc=0.8711 macroF1=0.8017 kappa=0.8211 AUROC=0.9707 AUPRC=0.8528 meanConf=0.7124 ECE=0.1587\n",
      "  MESA  : loss=1.5601 acc=0.7619 macroF1=0.6185 kappa=0.6588 AUROC=0.9023 AUPRC=0.6641 meanConf=0.6874 ECE=0.0745\n",
      "  MESA F1/class: {'W': 0.8793450110775021, 'N1': 0.37740074228861925, 'N2': 0.7805537498299259, 'N3': 0.2547466145469775, 'REM': 0.8003178244809042}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47 | train_loss=0.7701\n",
      "  VAL   : loss=0.8736 acc=0.8548 macroF1=0.7990 kappa=0.8000 AUROC=0.9681 AUPRC=0.8435 meanConf=0.7133 ECE=0.1417\n",
      "  F1/class: {'W': 0.913308347178754, 'N1': 0.5114737073355835, 'N2': 0.8640145325836068, 'N3': 0.8266421573420418, 'REM': 0.8794895416633176}\n",
      "  TEST1 : loss=0.8311 acc=0.8652 macroF1=0.8089 kappa=0.8137 AUROC=0.9717 AUPRC=0.8574 meanConf=0.7135 ECE=0.1517\n",
      "  SHHS2 : loss=0.8047 acc=0.8686 macroF1=0.7992 kappa=0.8180 AUROC=0.9700 AUPRC=0.8508 meanConf=0.7117 ECE=0.1569\n",
      "  MESA  : loss=1.5459 acc=0.7620 macroF1=0.6196 kappa=0.6594 AUROC=0.9040 AUPRC=0.6667 meanConf=0.6848 ECE=0.0773\n",
      "  MESA F1/class: {'W': 0.8796865036049296, 'N1': 0.37768913962980016, 'N2': 0.7814192143936353, 'N3': 0.26129314510336543, 'REM': 0.7978030359758083}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48 | train_loss=0.7643\n",
      "  VAL   : loss=0.8801 acc=0.8556 macroF1=0.7995 kappa=0.8008 AUROC=0.9676 AUPRC=0.8413 meanConf=0.7149 ECE=0.1408\n",
      "  F1/class: {'W': 0.9135645068132714, 'N1': 0.5114268066707843, 'N2': 0.8651511333837266, 'N3': 0.8271698511141722, 'REM': 0.8802275112234317}\n",
      "  TEST1 : loss=0.8347 acc=0.8660 macroF1=0.8093 kappa=0.8146 AUROC=0.9716 AUPRC=0.8565 meanConf=0.7148 ECE=0.1512\n",
      "  SHHS2 : loss=0.8080 acc=0.8698 macroF1=0.8000 kappa=0.8194 AUROC=0.9696 AUPRC=0.8501 meanConf=0.7134 ECE=0.1564\n",
      "  MESA  : loss=1.5930 acc=0.7565 macroF1=0.6115 kappa=0.6517 AUROC=0.8984 AUPRC=0.6487 meanConf=0.6870 ECE=0.0694\n",
      "  MESA F1/class: {'W': 0.8780706045773585, 'N1': 0.37516643044281234, 'N2': 0.7753677110505479, 'N3': 0.2272719611676667, 'REM': 0.8014887376364002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49 | train_loss=0.7588\n",
      "  VAL   : loss=0.8820 acc=0.8553 macroF1=0.7990 kappa=0.8003 AUROC=0.9675 AUPRC=0.8412 meanConf=0.7156 ECE=0.1397\n",
      "  F1/class: {'W': 0.9126463619819387, 'N1': 0.5102580940441367, 'N2': 0.865589884045358, 'N3': 0.8266779220607924, 'REM': 0.8796983592685461}\n",
      "  TEST1 : loss=0.8358 acc=0.8657 macroF1=0.8088 kappa=0.8142 AUROC=0.9717 AUPRC=0.8570 meanConf=0.7154 ECE=0.1504\n",
      "  SHHS2 : loss=0.8178 acc=0.8682 macroF1=0.7982 kappa=0.8172 AUROC=0.9690 AUPRC=0.8487 meanConf=0.7139 ECE=0.1543\n",
      "  MESA  : loss=1.6371 acc=0.7503 macroF1=0.6108 kappa=0.6436 AUROC=0.8946 AUPRC=0.6465 meanConf=0.6876 ECE=0.0628\n",
      "  MESA F1/class: {'W': 0.8675291938885745, 'N1': 0.37192055106659544, 'N2': 0.7737991610101216, 'N3': 0.23557165791983797, 'REM': 0.8053557968141011}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50 | train_loss=0.7544\n",
      "  VAL   : loss=0.8844 acc=0.8553 macroF1=0.7987 kappa=0.8003 AUROC=0.9674 AUPRC=0.8412 meanConf=0.7174 ECE=0.1379\n",
      "  F1/class: {'W': 0.9123589710528012, 'N1': 0.508675799086758, 'N2': 0.8658425334648967, 'N3': 0.827181905107323, 'REM': 0.879621230008326}\n",
      "  TEST1 : loss=0.8380 acc=0.8655 macroF1=0.8083 kappa=0.8139 AUROC=0.9714 AUPRC=0.8556 meanConf=0.7171 ECE=0.1484\n",
      "  SHHS2 : loss=0.8192 acc=0.8681 macroF1=0.7981 kappa=0.8170 AUROC=0.9688 AUPRC=0.8481 meanConf=0.7156 ECE=0.1525\n",
      "  MESA  : loss=1.6205 acc=0.7546 macroF1=0.6127 kappa=0.6490 AUROC=0.8984 AUPRC=0.6551 meanConf=0.6884 ECE=0.0662\n",
      "  MESA F1/class: {'W': 0.8722530968810555, 'N1': 0.37191213104853926, 'N2': 0.7742811661509993, 'N3': 0.24740533696666483, 'REM': 0.7977499118592695}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51 | train_loss=0.7479\n",
      "  VAL   : loss=0.8871 acc=0.8554 macroF1=0.7987 kappa=0.8003 AUROC=0.9672 AUPRC=0.8409 meanConf=0.7184 ECE=0.1370\n",
      "  F1/class: {'W': 0.9119350255201694, 'N1': 0.5086719030341693, 'N2': 0.8659974135444675, 'N3': 0.8270344135568156, 'REM': 0.8799736315128297}\n",
      "  TEST1 : loss=0.8401 acc=0.8657 macroF1=0.8081 kappa=0.8140 AUROC=0.9712 AUPRC=0.8553 meanConf=0.7182 ECE=0.1475\n",
      "  SHHS2 : loss=0.8194 acc=0.8684 macroF1=0.7984 kappa=0.8173 AUROC=0.9688 AUPRC=0.8481 meanConf=0.7163 ECE=0.1521\n",
      "  MESA  : loss=1.6016 acc=0.7637 macroF1=0.6182 kappa=0.6603 AUROC=0.9019 AUPRC=0.6626 meanConf=0.6935 ECE=0.0702\n",
      "  MESA F1/class: {'W': 0.8829846681282832, 'N1': 0.37244233416163985, 'N2': 0.7784511636362373, 'N3': 0.2588374306626842, 'REM': 0.7982962962962963}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52 | train_loss=0.7480\n",
      "  VAL   : loss=0.8877 acc=0.8542 macroF1=0.7976 kappa=0.7989 AUROC=0.9668 AUPRC=0.8397 meanConf=0.7179 ECE=0.1363\n",
      "  F1/class: {'W': 0.9115122156516662, 'N1': 0.5061936378951541, 'N2': 0.864346377085565, 'N3': 0.8263915742009058, 'REM': 0.8794810939771612}\n",
      "  TEST1 : loss=0.8402 acc=0.8648 macroF1=0.8075 kappa=0.8131 AUROC=0.9710 AUPRC=0.8543 meanConf=0.7178 ECE=0.1471\n",
      "  SHHS2 : loss=0.8222 acc=0.8671 macroF1=0.7972 kappa=0.8157 AUROC=0.9685 AUPRC=0.8469 meanConf=0.7158 ECE=0.1513\n",
      "  MESA  : loss=1.6115 acc=0.7615 macroF1=0.6183 kappa=0.6576 AUROC=0.9009 AUPRC=0.6618 meanConf=0.6932 ECE=0.0683\n",
      "  MESA F1/class: {'W': 0.8791134254792102, 'N1': 0.3718202500392514, 'N2': 0.777598164166584, 'N3': 0.2620407342769163, 'REM': 0.8008094049178764}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53 | train_loss=0.7362\n",
      "  VAL   : loss=0.8912 acc=0.8546 macroF1=0.7980 kappa=0.7993 AUROC=0.9665 AUPRC=0.8390 meanConf=0.7194 ECE=0.1352\n",
      "  F1/class: {'W': 0.91127425725571, 'N1': 0.5083118286022925, 'N2': 0.8648459791533221, 'N3': 0.8258118660625889, 'REM': 0.8798365852291482}\n",
      "  TEST1 : loss=0.8434 acc=0.8648 macroF1=0.8074 kappa=0.8129 AUROC=0.9706 AUPRC=0.8542 meanConf=0.7191 ECE=0.1457\n",
      "  SHHS2 : loss=0.8214 acc=0.8682 macroF1=0.7978 kappa=0.8171 AUROC=0.9683 AUPRC=0.8470 meanConf=0.7181 ECE=0.1500\n",
      "  MESA  : loss=1.6407 acc=0.7587 macroF1=0.6156 kappa=0.6535 AUROC=0.8978 AUPRC=0.6591 meanConf=0.6953 ECE=0.0633\n",
      "  MESA F1/class: {'W': 0.8756451208678944, 'N1': 0.3650170991752163, 'N2': 0.7762250552575676, 'N3': 0.26519932138272756, 'REM': 0.795991707466448}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54 | train_loss=0.7404\n",
      "  VAL   : loss=0.8944 acc=0.8542 macroF1=0.7976 kappa=0.7988 AUROC=0.9663 AUPRC=0.8383 meanConf=0.7196 ECE=0.1346\n",
      "  F1/class: {'W': 0.9114692268129607, 'N1': 0.5070608737455682, 'N2': 0.8643049561432127, 'N3': 0.8261595059781893, 'REM': 0.878778641151183}\n",
      "  TEST1 : loss=0.8464 acc=0.8644 macroF1=0.8066 kappa=0.8124 AUROC=0.9704 AUPRC=0.8532 meanConf=0.7194 ECE=0.1450\n",
      "  SHHS2 : loss=0.8267 acc=0.8674 macroF1=0.7969 kappa=0.8161 AUROC=0.9678 AUPRC=0.8452 meanConf=0.7182 ECE=0.1492\n",
      "  MESA  : loss=1.6397 acc=0.7573 macroF1=0.6154 kappa=0.6522 AUROC=0.8965 AUPRC=0.6551 meanConf=0.6954 ECE=0.0619\n",
      "  MESA F1/class: {'W': 0.8763210967017258, 'N1': 0.36596910347911965, 'N2': 0.7752956075482312, 'N3': 0.2646414399031236, 'REM': 0.794972436759436}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55 | train_loss=0.7388\n",
      "  VAL   : loss=0.8970 acc=0.8537 macroF1=0.7969 kappa=0.7981 AUROC=0.9661 AUPRC=0.8373 meanConf=0.7195 ECE=0.1342\n",
      "  F1/class: {'W': 0.9112729765628348, 'N1': 0.5052577073209591, 'N2': 0.8637195235783893, 'N3': 0.8252146372107633, 'REM': 0.8790746975932162}\n",
      "  TEST1 : loss=0.8477 acc=0.8639 macroF1=0.8060 kappa=0.8117 AUROC=0.9705 AUPRC=0.8523 meanConf=0.7191 ECE=0.1448\n",
      "  SHHS2 : loss=0.8283 acc=0.8669 macroF1=0.7964 kappa=0.8154 AUROC=0.9677 AUPRC=0.8446 meanConf=0.7183 ECE=0.1487\n",
      "  MESA  : loss=1.6373 acc=0.7585 macroF1=0.6180 kappa=0.6539 AUROC=0.8966 AUPRC=0.6533 meanConf=0.6965 ECE=0.0619\n",
      "  MESA F1/class: {'W': 0.8796449374175523, 'N1': 0.36907885147937614, 'N2': 0.7740414209235598, 'N3': 0.26462967849094465, 'REM': 0.8025423289549176}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56 | train_loss=0.7345\n",
      "  VAL   : loss=0.8967 acc=0.8544 macroF1=0.7974 kappa=0.7990 AUROC=0.9661 AUPRC=0.8373 meanConf=0.7199 ECE=0.1344\n",
      "  F1/class: {'W': 0.9113338737973603, 'N1': 0.5052182068525134, 'N2': 0.8647356835889731, 'N3': 0.8263716918792161, 'REM': 0.8794089430995601}\n",
      "  TEST1 : loss=0.8473 acc=0.8645 macroF1=0.8066 kappa=0.8125 AUROC=0.9704 AUPRC=0.8524 meanConf=0.7196 ECE=0.1449\n",
      "  SHHS2 : loss=0.8261 acc=0.8676 macroF1=0.7971 kappa=0.8163 AUROC=0.9678 AUPRC=0.8450 meanConf=0.7184 ECE=0.1492\n",
      "  MESA  : loss=1.6370 acc=0.7604 macroF1=0.6179 kappa=0.6561 AUROC=0.8973 AUPRC=0.6543 meanConf=0.6980 ECE=0.0624\n",
      "  MESA F1/class: {'W': 0.8801862803956113, 'N1': 0.36964501276258516, 'N2': 0.775996612285542, 'N3': 0.2578585894162191, 'REM': 0.8059122093898531}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57 | train_loss=0.7317\n",
      "  VAL   : loss=0.8965 acc=0.8544 macroF1=0.7975 kappa=0.7990 AUROC=0.9661 AUPRC=0.8374 meanConf=0.7200 ECE=0.1344\n",
      "  F1/class: {'W': 0.9113929477944708, 'N1': 0.5059710747934628, 'N2': 0.8647055259613456, 'N3': 0.8264935405600368, 'REM': 0.8788160149556005}\n",
      "  TEST1 : loss=0.8474 acc=0.8643 macroF1=0.8064 kappa=0.8122 AUROC=0.9703 AUPRC=0.8523 meanConf=0.7197 ECE=0.1446\n",
      "  SHHS2 : loss=0.8274 acc=0.8674 macroF1=0.7969 kappa=0.8160 AUROC=0.9676 AUPRC=0.8446 meanConf=0.7186 ECE=0.1488\n",
      "  MESA  : loss=1.6385 acc=0.7594 macroF1=0.6188 kappa=0.6552 AUROC=0.8974 AUPRC=0.6547 meanConf=0.6985 ECE=0.0609\n",
      "  MESA F1/class: {'W': 0.8789030321083758, 'N1': 0.37231641511385277, 'N2': 0.7756851238358365, 'N3': 0.26282667408918325, 'REM': 0.8040424204616344}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58 | train_loss=0.7312\n",
      "  VAL   : loss=0.8971 acc=0.8543 macroF1=0.7975 kappa=0.7990 AUROC=0.9660 AUPRC=0.8374 meanConf=0.7205 ECE=0.1339\n",
      "  F1/class: {'W': 0.9114835303746428, 'N1': 0.5070025188916877, 'N2': 0.8644828124599471, 'N3': 0.8256660658692446, 'REM': 0.8788082520947721}\n",
      "  TEST1 : loss=0.8477 acc=0.8644 macroF1=0.8065 kappa=0.8124 AUROC=0.9703 AUPRC=0.8525 meanConf=0.7201 ECE=0.1443\n",
      "  SHHS2 : loss=0.8288 acc=0.8673 macroF1=0.7968 kappa=0.8159 AUROC=0.9675 AUPRC=0.8445 meanConf=0.7191 ECE=0.1481\n",
      "  MESA  : loss=1.6617 acc=0.7556 macroF1=0.6136 kappa=0.6500 AUROC=0.8948 AUPRC=0.6494 meanConf=0.6985 ECE=0.0571\n",
      "  MESA F1/class: {'W': 0.874987230825808, 'N1': 0.366399392380676, 'N2': 0.7740154561004935, 'N3': 0.2549783010890235, 'REM': 0.7976458546571136}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59 | train_loss=0.7309\n",
      "  VAL   : loss=0.8973 acc=0.8538 macroF1=0.7970 kappa=0.7984 AUROC=0.9660 AUPRC=0.8371 meanConf=0.7199 ECE=0.1339\n",
      "  F1/class: {'W': 0.9113124927626979, 'N1': 0.5052021967049426, 'N2': 0.8638882534465614, 'N3': 0.8258085614310475, 'REM': 0.8787680902538079}\n",
      "  TEST1 : loss=0.8478 acc=0.8640 macroF1=0.8063 kappa=0.8119 AUROC=0.9702 AUPRC=0.8521 meanConf=0.7195 ECE=0.1445\n",
      "  SHHS2 : loss=0.8282 acc=0.8668 macroF1=0.7964 kappa=0.8153 AUROC=0.9676 AUPRC=0.8444 meanConf=0.7184 ECE=0.1484\n",
      "  MESA  : loss=1.6545 acc=0.7562 macroF1=0.6152 kappa=0.6509 AUROC=0.8955 AUPRC=0.6514 meanConf=0.6982 ECE=0.0580\n",
      "  MESA F1/class: {'W': 0.8754429959736342, 'N1': 0.36845183953350913, 'N2': 0.7737054114426044, 'N3': 0.25983416682903593, 'REM': 0.7986418076305717}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60 | train_loss=0.7269\n",
      "  VAL   : loss=0.8973 acc=0.8542 macroF1=0.7974 kappa=0.7988 AUROC=0.9661 AUPRC=0.8373 meanConf=0.7203 ECE=0.1339\n",
      "  F1/class: {'W': 0.9114018435272994, 'N1': 0.50579814459373, 'N2': 0.8643699493124304, 'N3': 0.8260416051642361, 'REM': 0.879179170454121}\n",
      "  TEST1 : loss=0.8480 acc=0.8642 macroF1=0.8063 kappa=0.8121 AUROC=0.9703 AUPRC=0.8523 meanConf=0.7200 ECE=0.1442\n",
      "  SHHS2 : loss=0.8272 acc=0.8675 macroF1=0.7969 kappa=0.8161 AUROC=0.9676 AUPRC=0.8446 meanConf=0.7189 ECE=0.1486\n",
      "  MESA  : loss=1.6603 acc=0.7564 macroF1=0.6138 kappa=0.6510 AUROC=0.8951 AUPRC=0.6499 meanConf=0.6988 ECE=0.0576\n",
      "  MESA F1/class: {'W': 0.8754923565608126, 'N1': 0.36817944384698115, 'N2': 0.7744424761581208, 'N3': 0.252421341822384, 'REM': 0.7983495556495981}\n",
      "\n",
      "==============================\n",
      "Training finished\n",
      "==============================\n",
      "BEST VAL macroF1 : 0.8019 | /data2/Akbar1/sleep_stages_Dibatic/shhs_sleepstaging_planA/checkpoints_hier_rope_seq_v5_1/BEST_VAL_macroF1.pt\n",
      "BEST MESA macroF1: 0.7011 | /data2/Akbar1/sleep_stages_Dibatic/shhs_sleepstaging_planA/checkpoints_hier_rope_seq_v5_1/BEST_MESA_macroF1.pt\n",
      "\n",
      "Top-K VAL checkpoints for ensemble:\n",
      " - /data2/Akbar1/sleep_stages_Dibatic/shhs_sleepstaging_planA/checkpoints_hier_rope_seq_v5_1/VALBEST_ep018_valF1_0.7960.pt\n",
      " - /data2/Akbar1/sleep_stages_Dibatic/shhs_sleepstaging_planA/checkpoints_hier_rope_seq_v5_1/VALBEST_ep019_valF1_0.7973.pt\n",
      " - /data2/Akbar1/sleep_stages_Dibatic/shhs_sleepstaging_planA/checkpoints_hier_rope_seq_v5_1/VALBEST_ep021_valF1_0.7981.pt\n",
      " - /data2/Akbar1/sleep_stages_Dibatic/shhs_sleepstaging_planA/checkpoints_hier_rope_seq_v5_1/VALBEST_ep023_valF1_0.7995.pt\n",
      " - /data2/Akbar1/sleep_stages_Dibatic/shhs_sleepstaging_planA/checkpoints_hier_rope_seq_v5_1/VALBEST_ep029_valF1_0.8019.pt\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "# -------------------------\n",
    "# Training loop + checkpoints\n",
    "# - Save BEST by VAL macro-F1 (main)\n",
    "# - Save BEST by MESA macro-F1 (secondary; if MESA loader exists)\n",
    "# - Keep Top-K by VAL macro-F1 for ensemble\n",
    "# -------------------------\n",
    "CKPT_DIR = ROOT / \"checkpoints_hier_rope_seq_v5_1\"   # <= new folder to avoid overwriting old runs\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_val_path  = CKPT_DIR / \"BEST_VAL_macroF1.pt\"\n",
    "best_mesa_path = CKPT_DIR / \"BEST_MESA_macroF1.pt\"\n",
    "\n",
    "best_val  = -1.0\n",
    "best_mesa = -1.0\n",
    "\n",
    "topk_paths = []\n",
    "ENSEMBLE_K = 5\n",
    "\n",
    "def _make_payload(epoch, tr_loss, val_m, test_m, ext_m, mesa_m):\n",
    "    payload = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": float(tr_loss),\n",
    "\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "\n",
    "        \"val_metrics\": val_m,\n",
    "        \"test_metrics\": test_m,\n",
    "        \"ext_metrics\": ext_m,\n",
    "        \"mesa_metrics\": mesa_m,\n",
    "\n",
    "        \"class_weights\": class_weights.detach().cpu().numpy(),\n",
    "\n",
    "        \"use_ema\": USE_EMA,\n",
    "        \"ema_decay\": EMA_DECAY,\n",
    "        \"ema_shadow\": {k: v.detach().cpu() for k, v in ema.shadow.items()} if USE_EMA else None,\n",
    "\n",
    "        \"use_learned_smoothing\": USE_LEARNED_SMOOTHING,\n",
    "        \"use_viterbi\": USE_VITERBI,\n",
    "        \"Tmat\": Tmat if USE_VITERBI else None,\n",
    "\n",
    "        # V5 config snapshot (works for V5.1 too)\n",
    "        \"v5_soft_boundary\": dict(enabled=USE_SOFT_BOUNDARY_LOSS, weight=SOFT_BOUNDARY_WEIGHT),\n",
    "        \"v5_cost_matrix\": dict(enabled=USE_COST_MATRIX, weight=COST_WEIGHT),\n",
    "        \"v5_aux_dur\": dict(enabled=USE_AUX_DUR, weight=AUX_DUR_WEIGHT, n1_mult=AUX_DUR_N1_MULT, dur_edges=DUR_EDGES),\n",
    "    }\n",
    "    return payload\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss = train_one_epoch(model, train_seq_loader, epoch)\n",
    "\n",
    "    # Evaluate with EMA weights\n",
    "    if USE_EMA:\n",
    "        ema.apply(model)\n",
    "\n",
    "    val_m  = eval_sequence(model, val_seq_loader,  desc=\"VAL\")\n",
    "    test_m = eval_sequence(model, test_seq_loader, desc=\"SHHS1 TEST\")\n",
    "    ext_m  = eval_sequence(model, ext_seq_loader,  desc=\"SHHS2 EXT\")\n",
    "    mesa_m = eval_sequence(model, mesa_seq_loader, desc=\"MESA EXT\") if mesa_seq_loader is not None else None\n",
    "\n",
    "    if USE_EMA:\n",
    "        ema.restore(model)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch:02d} | train_loss={tr_loss:.4f}\")\n",
    "\n",
    "    print(f\"  VAL   : loss={val_m['loss']:.4f} acc={val_m['acc']:.4f} macroF1={val_m['macro_f1']:.4f} \"\n",
    "          f\"kappa={val_m['kappa']:.4f} AUROC={val_m['AUROC']:.4f} AUPRC={val_m['AUPRC']:.4f} \"\n",
    "          f\"meanConf={val_m['meanConf']:.4f} ECE={val_m['ECE']:.4f}\")\n",
    "    print(f\"  F1/class: {val_m['f1_per_class']}\")\n",
    "\n",
    "    print(f\"  TEST1 : loss={test_m['loss']:.4f} acc={test_m['acc']:.4f} macroF1={test_m['macro_f1']:.4f} \"\n",
    "          f\"kappa={test_m['kappa']:.4f} AUROC={test_m['AUROC']:.4f} AUPRC={test_m['AUPRC']:.4f} \"\n",
    "          f\"meanConf={test_m['meanConf']:.4f} ECE={test_m['ECE']:.4f}\")\n",
    "\n",
    "    print(f\"  SHHS2 : loss={ext_m['loss']:.4f} acc={ext_m['acc']:.4f} macroF1={ext_m['macro_f1']:.4f} \"\n",
    "          f\"kappa={ext_m['kappa']:.4f} AUROC={ext_m['AUROC']:.4f} AUPRC={ext_m['AUPRC']:.4f} \"\n",
    "          f\"meanConf={ext_m['meanConf']:.4f} ECE={ext_m['ECE']:.4f}\")\n",
    "\n",
    "    if mesa_m is not None:\n",
    "        print(f\"  MESA  : loss={mesa_m['loss']:.4f} acc={mesa_m['acc']:.4f} macroF1={mesa_m['macro_f1']:.4f} \"\n",
    "              f\"kappa={mesa_m['kappa']:.4f} AUROC={mesa_m['AUROC']:.4f} AUPRC={mesa_m['AUPRC']:.4f} \"\n",
    "              f\"meanConf={mesa_m['meanConf']:.4f} ECE={mesa_m['ECE']:.4f}\")\n",
    "        print(f\"  MESA F1/class: {mesa_m['f1_per_class']}\")\n",
    "\n",
    "    if USE_VITERBI:\n",
    "        print(f\"  VAL(viterbi): acc={val_m['viterbi_acc']:.4f} macroF1={val_m['viterbi_macro_f1']:.4f} kappa={val_m['viterbi_kappa']:.4f}\")\n",
    "\n",
    "    payload = _make_payload(epoch, tr_loss, val_m, test_m, ext_m, mesa_m)\n",
    "\n",
    "    # ============================================================\n",
    "    # (A) Save BEST by VAL macro-F1 (main)\n",
    "    # ============================================================\n",
    "    if (val_m is not None) and (val_m.get(\"macro_f1\", float(\"nan\")) == val_m.get(\"macro_f1\", float(\"nan\"))):\n",
    "        if val_m[\"macro_f1\"] > best_val:\n",
    "            best_val = float(val_m[\"macro_f1\"])\n",
    "            payload[\"best_val_macroF1\"] = best_val\n",
    "            torch.save(payload, best_val_path)\n",
    "            print(\"  ? Saved BEST_VAL:\", best_val_path.name, f\"(val_macroF1={best_val:.4f})\")\n",
    "\n",
    "            ck = CKPT_DIR / f\"VALBEST_ep{epoch:03d}_valF1_{best_val:.4f}.pt\"\n",
    "            torch.save(payload, ck)\n",
    "            topk_paths.append(ck)\n",
    "            topk_paths = topk_paths[-ENSEMBLE_K:]\n",
    "            print(\"  ? Added to VAL Top-K:\", ck.name)\n",
    "\n",
    "    # ============================================================\n",
    "    # (B) Save BEST by MESA macro-F1 (secondary)\n",
    "    # ============================================================\n",
    "    if mesa_m is not None:\n",
    "        mf1 = mesa_m.get(\"macro_f1\", float(\"nan\"))\n",
    "        if mf1 == mf1:  # not NaN\n",
    "            if mf1 > best_mesa:\n",
    "                best_mesa = float(mf1)\n",
    "                payload[\"best_mesa_macroF1\"] = best_mesa\n",
    "                torch.save(payload, best_mesa_path)\n",
    "                print(\"  ? Saved BEST_MESA:\", best_mesa_path.name, f\"(mesa_macroF1={best_mesa:.4f})\")\n",
    "\n",
    "                ck2 = CKPT_DIR / f\"MESABEST_ep{epoch:03d}_mesaF1_{best_mesa:.4f}.pt\"\n",
    "                torch.save(payload, ck2)\n",
    "                print(\"  ? Snapshot MESA best:\", ck2.name)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Training finished\")\n",
    "print(\"==============================\")\n",
    "print(\"BEST VAL macroF1 :\", f\"{best_val:.4f}\", \"|\", best_val_path)\n",
    "if mesa_seq_loader is not None:\n",
    "    print(\"BEST MESA macroF1:\", f\"{best_mesa:.4f}\", \"|\", best_mesa_path)\n",
    "\n",
    "print(\"\\nTop-K VAL checkpoints for ensemble:\")\n",
    "for p in topk_paths:\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7db62b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 0: Imports (safe)\n",
    "# =========================\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "128f5778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: /data2/Akbar1/sleep_stages_Dibatic/shhs_sleepstaging_planA/checkpoints_hier_rope_seq_v5_1/BEST_VAL_macroF1.pt\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 1: Paths (EDIT ONLY THIS CELL IF NEEDED)\n",
    "# ============================================\n",
    "ROOT = Path(\"/data2/Akbar1/sleep_stages_Dibatic/shhs_sleepstaging_planA/\")\n",
    "\n",
    "# Your training log shows checkpoints in:\n",
    "# /data2/Akbar1/.../checkpoints_hier_rope_seq_v5_1/\n",
    "# so default to that (change if needed).\n",
    "CKPT_DIR = ROOT / \"checkpoints_hier_rope_seq_v5_1\"\n",
    "\n",
    "# If you want to force a specific file, set BEST_CKPT explicitly.\n",
    "# Otherwise it will auto-pick BEST_VAL_macroF1.pt if exists.\n",
    "BEST_CKPT = CKPT_DIR / \"BEST_VAL_macroF1.pt\"\n",
    "\n",
    "assert CKPT_DIR.exists(), f\"Checkpoint folder not found: {CKPT_DIR}\"\n",
    "\n",
    "if not BEST_CKPT.exists():\n",
    "    # fallback: search common best names\n",
    "    candidates = [\n",
    "        CKPT_DIR / \"BEST_VAL_macroF1.pt\",\n",
    "        CKPT_DIR / \"V5_best_val_macroF1.pt\",\n",
    "        CKPT_DIR / \"best.pt\",\n",
    "        CKPT_DIR / \"checkpoint_best.pt\",\n",
    "    ]\n",
    "    found = None\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            found = c\n",
    "            break\n",
    "    if found is None:\n",
    "        # last fallback: pick newest .pt\n",
    "        pts = sorted(CKPT_DIR.glob(\"*.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        assert len(pts) > 0, f\"No .pt checkpoints found in: {CKPT_DIR}\"\n",
    "        found = pts[0]\n",
    "    BEST_CKPT = found\n",
    "\n",
    "print(\"Using checkpoint:\", BEST_CKPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31cc7897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: ['Tmat', 'best_val_macroF1', 'class_weights', 'ema_decay', 'ema_shadow', 'epoch', 'ext_metrics', 'mesa_metrics', 'model_state', 'optimizer_state', 'test_metrics', 'train_loss', 'use_ema', 'use_learned_smoothing', 'use_viterbi', 'v5_aux_dur', 'v5_cost_matrix', 'v5_soft_boundary', 'val_metrics'] ...\n",
      "Has model_state: True\n",
      "Has state_dict: False\n",
      "Has model object: False\n",
      "Has ema_shadow: True\n",
      "use_ema flag: True\n",
      "arch/model_class: None None\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 2: Load checkpoint (robust)\n",
    "# ============================================\n",
    "ckpt = torch.load(BEST_CKPT, map_location=\"cpu\")\n",
    "\n",
    "print(\"Checkpoint keys:\", sorted(list(ckpt.keys()))[:40], \"...\")\n",
    "print(\"Has model_state:\", \"model_state\" in ckpt)\n",
    "print(\"Has state_dict:\", \"state_dict\" in ckpt)\n",
    "print(\"Has model object:\", \"model\" in ckpt)\n",
    "print(\"Has ema_shadow:\", \"ema_shadow\" in ckpt)\n",
    "print(\"use_ema flag:\", ckpt.get(\"use_ema\", False))\n",
    "print(\"arch/model_class:\", ckpt.get(\"arch\", None), ckpt.get(\"model_class\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c5c2f7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "device not found. Make sure you defined: device = torch.device('cuda'...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_475896/706904463.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# ---- build model now ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m\"device\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"device not found. Make sure you defined: device = torch.device('cuda'...)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m\"NUM_CLASSES\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NUM_CLASSES not found. Make sure NUM_CLASSES and LABELS are defined.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: device not found. Make sure you defined: device = torch.device('cuda'...)"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Build / recover model WITHOUT hardcoding class name\n",
    "# ============================================================\n",
    "\n",
    "def _find_candidate_model_classes(globs: dict):\n",
    "    \"\"\"\n",
    "    Heuristic: find classes that likely represent your model.\n",
    "    Priority: names containing V5, Sleep, Transformer, Hier, etc.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for name, obj in globs.items():\n",
    "        if isinstance(obj, type):  # is a class\n",
    "            n = name.lower()\n",
    "            if any(k in n for k in [\"sleep\", \"transformer\", \"hier\", \"stage\", \"v5\"]):\n",
    "                candidates.append(name)\n",
    "\n",
    "    def score(nm: str):\n",
    "        n = nm.lower()\n",
    "        s = 0\n",
    "        if \"v5\" in n: s += 10\n",
    "        if \"hier\" in n: s += 6\n",
    "        if \"sleep\" in n: s += 6\n",
    "        if \"transformer\" in n: s += 6\n",
    "        if \"net\" in n: s += 2\n",
    "        return s\n",
    "\n",
    "    candidates = sorted(set(candidates), key=lambda x: score(x), reverse=True)\n",
    "    return candidates\n",
    "\n",
    "def _build_model_from_notebook_or_ckpt(ckpt: dict):\n",
    "    \"\"\"\n",
    "    Build model using (in order):\n",
    "      A) ckpt[\"model\"] object\n",
    "      B) build_model()/make_model()/get_model() if exists\n",
    "      C) ckpt[\"model_class\"]/ckpt[\"arch\"] class name\n",
    "      D) autodetect from globals\n",
    "    \"\"\"\n",
    "    # A) checkpoint contains full model object\n",
    "    if \"model\" in ckpt and ckpt[\"model\"] is not None:\n",
    "        m = ckpt[\"model\"]\n",
    "        if isinstance(m, torch.nn.Module):\n",
    "            print(\"[Model] Using ckpt['model'] object directly.\")\n",
    "            return m\n",
    "        else:\n",
    "            print(\"[Model] ckpt['model'] exists but is not a torch.nn.Module. Ignoring.\")\n",
    "\n",
    "    # B) common builder functions in notebook\n",
    "    for fn_name in [\"build_model\", \"make_model\", \"get_model\"]:\n",
    "        fn = globals().get(fn_name, None)\n",
    "        if callable(fn):\n",
    "            try:\n",
    "                print(f\"[Model] Using notebook builder: {fn_name}()\")\n",
    "                return fn()\n",
    "            except TypeError:\n",
    "                # some builders require args; fall through\n",
    "                print(f\"[Model] Found {fn_name} but it needs args. Will try other methods.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Model] {fn_name}() failed: {e}. Will try other methods.\")\n",
    "\n",
    "    # C) checkpoint tells class name\n",
    "    for key in [\"model_class\", \"arch\"]:\n",
    "        cls_name = ckpt.get(key, None)\n",
    "        if isinstance(cls_name, str) and cls_name in globals() and isinstance(globals()[cls_name], type):\n",
    "            cls = globals()[cls_name]\n",
    "            print(f\"[Model] Using class from ckpt['{key}']:\", cls_name)\n",
    "            # Try instantiate with common patterns\n",
    "            # NOTE: adjust here if your constructor is very custom\n",
    "            try:\n",
    "                return cls(num_classes=NUM_CLASSES).to(device)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    return cls(NUM_CLASSES).to(device)\n",
    "                except Exception:\n",
    "                    # last attempt: no args\n",
    "                    return cls().to(device)\n",
    "\n",
    "    # D) auto-detect likely class names in notebook globals\n",
    "    candidates = _find_candidate_model_classes(globals())\n",
    "    if len(candidates) == 0:\n",
    "        raise NameError(\n",
    "            \"No candidate model class found in notebook globals.\\n\"\n",
    "            \"Make sure you executed the cell that defines your model class (V5) before running evaluation.\"\n",
    "        )\n",
    "\n",
    "    print(\"[Model] Auto-detected candidate classes (top 10):\", candidates[:10])\n",
    "    picked = candidates[0]\n",
    "    cls = globals()[picked]\n",
    "    print(\"[Model] Picking:\", picked)\n",
    "\n",
    "    # Try instantiate with your earlier hyperparams (edit ONLY if needed)\n",
    "    # These are safe tries; if fails, it will attempt simpler calls.\n",
    "    try:\n",
    "        return cls(num_classes=NUM_CLASSES, d_model=384, depth=10, n_heads=8).to(device)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return cls(num_classes=NUM_CLASSES).to(device)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return cls(NUM_CLASSES).to(device)\n",
    "            except Exception:\n",
    "                return cls().to(device)\n",
    "\n",
    "# ---- build model now ----\n",
    "assert \"device\" in globals(), \"device not found. Make sure you defined: device = torch.device('cuda'...)\"\n",
    "assert \"NUM_CLASSES\" in globals(), \"NUM_CLASSES not found. Make sure NUM_CLASSES and LABELS are defined.\"\n",
    "\n",
    "model = _build_model_from_notebook_or_ckpt(ckpt)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model type:\", type(model).__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2de2a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_475896/229634885.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded weights.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  missing keys   :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 4: Load weights + apply EMA if present\n",
    "# ============================================\n",
    "def _get_state_dict(ckpt: dict):\n",
    "    if \"model_state\" in ckpt and isinstance(ckpt[\"model_state\"], dict):\n",
    "        return ckpt[\"model_state\"]\n",
    "    if \"state_dict\" in ckpt and isinstance(ckpt[\"state_dict\"], dict):\n",
    "        return ckpt[\"state_dict\"]\n",
    "    # sometimes saved as \"model\"->state_dict only, but we already tried model object above\n",
    "    raise KeyError(\"No 'model_state' or 'state_dict' found in checkpoint.\")\n",
    "\n",
    "sd = _get_state_dict(ckpt)\n",
    "\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "print(\"Loaded weights.\")\n",
    "print(\"  missing keys   :\", len(missing))\n",
    "print(\"  unexpected keys:\", len(unexpected))\n",
    "\n",
    "# Apply EMA shadow weights if present\n",
    "use_ema = bool(ckpt.get(\"use_ema\", False))\n",
    "ema_shadow = ckpt.get(\"ema_shadow\", None)\n",
    "\n",
    "if use_ema and isinstance(ema_shadow, dict) and len(ema_shadow) > 0:\n",
    "    with torch.no_grad():\n",
    "        curr = model.state_dict()\n",
    "        applied = 0\n",
    "        for k, v in ema_shadow.items():\n",
    "            if k in curr:\n",
    "                curr[k].copy_(v.to(curr[k].device).to(curr[k].dtype))\n",
    "                applied += 1\n",
    "        model.load_state_dict(curr, strict=False)\n",
    "    print(f\"Applied EMA shadow weights: {applied} tensors\")\n",
    "else:\n",
    "    print(\"EMA weights not found / not used.\")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7db6562b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== VAL =====\n",
      "loss     : 0.8704\n",
      "acc      : 0.8593\n",
      "macro_f1 : 0.8023\n",
      "kappa    : 0.8048\n",
      "AUROC    : 0.9705\n",
      "AUPRC    : 0.8486\n",
      "meanConf : 0.6920\n",
      "ECE      : 0.1673\n",
      "bad_batches: 0\n",
      "F1/class : {'W': 0.9133298332330255, 'N1': 0.5192242833052276, 'N2': 0.8697101058497362, 'N3': 0.8263279694311394, 'REM': 0.8830118850982974}\n",
      "Confusion Matrix (rows=true, cols=pred) labels: ['W', 'N1', 'N2', 'N3', 'REM']\n",
      "[[106987   5595   4747    501   1808]\n",
      " [  2458  12316   3538      8   1715]\n",
      " [  4002   7452 188691  14192   8814]\n",
      " [   238     14  10567  61416    147]\n",
      " [   956   2028   3223    149  71101]]\n",
      "\n",
      "===== SHHS1 TEST =====\n",
      "loss     : 0.8333\n",
      "acc      : 0.8686\n",
      "macro_f1 : 0.8119\n",
      "kappa    : 0.8175\n",
      "AUROC    : 0.9738\n",
      "AUPRC    : 0.8619\n",
      "meanConf : 0.6915\n",
      "ECE      : 0.1771\n",
      "bad_batches: 0\n",
      "F1/class : {'W': 0.9209574694378058, 'N1': 0.5344476681245398, 'N2': 0.8784149801573615, 'N3': 0.8341778076226524, 'REM': 0.8915967122465064}\n",
      "Confusion Matrix (rows=true, cols=pred) labels: ['W', 'N1', 'N2', 'N3', 'REM']\n",
      "[[110403   5530   4157    396   1927]\n",
      " [  2136  12342   3691     11   1665]\n",
      " [  3824   6633 191021  13922   8139]\n",
      " [   107      6   9812  61491    159]\n",
      " [   874   1830   2702     34  71268]]\n",
      "\n",
      "===== SHHS2 EXT =====\n",
      "loss     : 0.8102\n",
      "acc      : 0.8724\n",
      "macro_f1 : 0.8034\n",
      "kappa    : 0.8225\n",
      "AUROC    : 0.9744\n",
      "AUPRC    : 0.8595\n",
      "meanConf : 0.6789\n",
      "ECE      : 0.1936\n",
      "bad_batches: 0\n",
      "F1/class : {'W': 0.9256897852148824, 'N1': 0.49930466707459537, 'N2': 0.8798958517564988, 'N3': 0.8156556905264871, 'REM': 0.8963635667502813}\n",
      "Confusion Matrix (rows=true, cols=pred) labels: ['W', 'N1', 'N2', 'N3', 'REM']\n",
      "[[832726  43518  38634   7495  14644]\n",
      " [  8342  62832  25324    134   8986]\n",
      " [ 15817  27829 965156  79411  20875]\n",
      " [   389     22  33899 270668    253]\n",
      " [  4856  11859  21695    743 362878]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== MESA EXT =====\n",
      "loss     : 1.4322\n",
      "acc      : 0.7700\n",
      "macro_f1 : 0.6181\n",
      "kappa    : 0.6656\n",
      "AUROC    : 0.9228\n",
      "AUPRC    : 0.7178\n",
      "meanConf : 0.6281\n",
      "ECE      : 0.1419\n",
      "bad_batches: 0\n",
      "F1/class : {'W': 0.8713183480964034, 'N1': 0.38780251316123404, 'N2': 0.7856198036341492, 'N3': 0.24597317049716316, 'REM': 0.7999503681321465}\n",
      "Confusion Matrix (rows=true, cols=pred) labels: ['W', 'N1', 'N2', 'N3', 'REM']\n",
      "[[685027  22805  80228    885  41018]\n",
      " [ 24112  59594  84645     18  18821]\n",
      " [ 24274  33508 708261   4150  27705]\n",
      " [  1500     55 117830  20463    929]\n",
      " [  7516   4190  14201     91 228871]]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Evaluate (uses your existing eval_sequence function)\n",
    "# ============================================================\n",
    "assert \"eval_sequence\" in globals(), (\n",
    "    \"eval_sequence() not found in globals.\\n\"\n",
    "    \"Run the cell where you defined eval_sequence before this evaluation cell.\"\n",
    ")\n",
    "\n",
    "def _print_metrics(tag, m):\n",
    "    print(f\"\\n===== {tag} =====\")\n",
    "    for k in [\"loss\",\"acc\",\"macro_f1\",\"kappa\",\"AUROC\",\"AUPRC\",\"meanConf\",\"ECE\"]:\n",
    "        if k in m:\n",
    "            print(f\"{k:9s}: {m[k]:.4f}\")\n",
    "    if \"bad_batches\" in m:\n",
    "        print(\"bad_batches:\", m[\"bad_batches\"])\n",
    "    if \"f1_per_class\" in m:\n",
    "        print(\"F1/class :\", m[\"f1_per_class\"])\n",
    "    if \"cm\" in m:\n",
    "        labs = [LABELS[i] for i in range(NUM_CLASSES)] if \"LABELS\" in globals() else list(range(NUM_CLASSES))\n",
    "        print(\"Confusion Matrix (rows=true, cols=pred) labels:\", labs)\n",
    "        print(m[\"cm\"])\n",
    "\n",
    "# Required loaders must exist\n",
    "for nm in [\"val_seq_loader\", \"test_seq_loader\", \"ext_seq_loader\"]:\n",
    "    assert nm in globals(), f\"{nm} not found. Please create it before running evaluation.\"\n",
    "\n",
    "val_m  = eval_sequence(model, val_seq_loader,  desc=\"VAL\")\n",
    "test_m = eval_sequence(model, test_seq_loader, desc=\"SHHS1 TEST\")\n",
    "ext_m  = eval_sequence(model, ext_seq_loader,  desc=\"SHHS2 EXT\")\n",
    "\n",
    "_print_metrics(\"VAL\", val_m)\n",
    "_print_metrics(\"SHHS1 TEST\", test_m)\n",
    "_print_metrics(\"SHHS2 EXT\", ext_m)\n",
    "\n",
    "# Optional MESA\n",
    "if \"mesa_seq_loader\" in globals():\n",
    "    mesa_m = eval_sequence(model, mesa_seq_loader, desc=\"MESA EXT\")\n",
    "    _print_metrics(\"MESA EXT\", mesa_m)\n",
    "else:\n",
    "    print(\"\\n[MESA] mesa_seq_loader not found. (Skip)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "828b88a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
